# -*- coding: utf-8 -*-
"""
Created on Thu Jun 27 17:53:41 2024

@author: Criss
"""

import os

os.chdir('C:\\Users\\Criss\\Documents\\Lavoro\\Assegno_2024_2025\\Codici')

#Imports 
import xarray as xr
import numpy as np
import pandas as pd
import warnings
import minisom
import pickle
from minisom import asymptotic_decay
import cartopy.mpl.ticker as cticker
from cartopy.util import add_cyclic_point
from mpl_toolkits.mplot3d import Axes3D
import matplotlib as mpl
from numpy import savetxt
from numpy import loadtxt
import cartopy
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import glob
import matplotlib.pyplot as plt
import os
from itertools import product
import winsound
from datetime import date

#Functions Used in the Code
def getList(dict):
    list = []
    for key in dict.keys():
        list.append(key)
        
    return list
def sammon(x, n, display = 2, inputdist = 'raw', maxhalves = 20, maxiter = 500, tolfun = 1e-9, init = 'default'):

    import numpy as np 
    from scipy.spatial.distance import cdist

    """Perform Sammon mapping on dataset x
    y = sammon(x) applies the Sammon nonlinear mapping procedure on
    multivariate data x, where each row represents a pattern and each column
    represents a feature.  On completion, y contains the corresponding
    co-ordinates of each point on the map.  By default, a two-dimensional
    map is created.  Note if x contains any duplicated rows, SAMMON will
    fail (ungracefully). 
    [y,E] = sammon(x) also returns the value of the cost function in E (i.e.
    the stress of the mapping).
    An N-dimensional output map is generated by y = sammon(x,n) .
    A set of optimisation options can be specified using optional
    arguments, y = sammon(x,n,[OPTS]):
       maxiter        - maximum number of iterations
       tolfun         - relative tolerance on objective function
       maxhalves      - maximum number of step halvings
       input          - {'raw','distance'} if set to 'distance', X is 
                        interpreted as a matrix of pairwise distances.
       display        - 0 to 2. 0 least verbose, 2 max verbose.
       init           - {'pca', 'cmdscale', random', 'default'}
                        default is 'pca' if input is 'raw', 
                        'msdcale' if input is 'distance'
    The default options are retrieved by calling sammon(x) with no
    parameters.
    File        : sammon.py
    Date        : 18 April 2014
    Authors     : Tom J. Pollard (tom.pollard.11@ucl.ac.uk)
                : Ported from MATLAB implementation by 
                  Gavin C. Cawley and Nicola L. C. Talbot
    Description : Simple python implementation of Sammon's non-linear
                  mapping algorithm [1].
    References  : [1] Sammon, John W. Jr., "A Nonlinear Mapping for Data
                  Structure Analysis", IEEE Transactions on Computers,
                  vol. C-18, no. 5, pp 401-409, May 1969.
    Copyright   : (c) Dr Gavin C. Cawley, November 2007.
    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program; if not, write to the Free Software
    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
    """

    # Create distance matrix unless given by parameters
    if inputdist == 'distance':
        D = x
        if init == 'default':
            init = 'cmdscale'
    else:
        D = cdist(x, x)
        if init == 'default':
            init = 'pca'

    if inputdist == 'distance' and init == 'pca':
        raise ValueError("Cannot use init == 'pca' when inputdist == 'distance'")

    if np.count_nonzero(np.diagonal(D)) > 0:
        raise ValueError("The diagonal of the dissimilarity matrix must be zero")

    # Remaining initialisation
    N = x.shape[0]
    scale = 0.5 / D.sum()
    D = D + np.eye(N)     
    if np.count_nonzero(D<=0) > 0:
        raise ValueError("Off-diagonal dissimilarities must be strictly positive")   

    Dinv = 1 / D
    if init == 'pca':
        [UU,DD,_] = np.linalg.svd(x)
        y = UU[:,:n]*DD[:n] 
    elif init == 'cmdscale':
        from cmdscale import cmdscale
        y,e = cmdscale(D)
        y = y[:,:n]
    else:
        y = np.random.normal(0.0,1.0,[N,n])
    one = np.ones([N,n])
    d = cdist(y,y) + np.eye(N)
    dinv = 1. / d
    delta = D-d 
    E = ((delta**2)*Dinv).sum() 

    # Get on with it
    for i in range(maxiter):

        # Compute gradient, Hessian and search direction (note it is actually
        # 1/4 of the gradient and Hessian, but the step size is just the ratio
        # of the gradient and the diagonal of the Hessian so it doesn't
        # matter).
        delta = dinv - Dinv
        deltaone = np.dot(delta,one)
        g = np.dot(delta,y) - (y * deltaone)
        dinv3 = dinv ** 3
        y2 = y ** 2
        H = np.dot(dinv3,y2) - deltaone - np.dot(2,y) * np.dot(dinv3,y) + y2 * np.dot(dinv3,one)
        s = -g.flatten(order='F') / np.abs(H.flatten(order='F'))
        y_old    = y

        # Use step-halving procedure to ensure progress is made
        for j in range(maxhalves):
            s_reshape = np.reshape(s, (-1,n),order='F')
            y = y_old + s_reshape
            d = cdist(y, y) + np.eye(N)
            dinv = 1 / d
            delta = D - d
            E_new = ((delta**2)*Dinv).sum()
            if E_new < E:
                break
            else:
                s = 0.5*s

        # Bomb out if too many halving steps are required
        if j == maxhalves-1:
            print('Warning: maxhalves exceeded. Sammon mapping may not converge...')

        # Evaluate termination criterion
        if abs((E - E_new) / E) < tolfun:
            if display:
                print('TolFun exceeded: Optimisation terminated')
            break

        # Report progress
        E = E_new
        if display > 1:
            print('epoch = %d : E = %12.10f'% (i+1, E * scale))

    if i == maxiter-1:
        print('Warning: maxiter exceeded. Sammon mapping may not have converged...')

    # Fiddle stress to match the original Sammon paper
    E = E * scale
    
    return [y,E]

#my functions 

def divide_dates_by_index(list_of_dates, list_of_indices, number_indices):
    # Initialize a list of empty lists
    divided_dates = [[] for _ in range(number_indices)]
    
    # Iterate through dates and indices
    for date, index in zip(list_of_dates, list_of_indices):
        # Add the date to the list corresponding to the index
        divided_dates[index].append(date)
    
    return divided_dates

# COLORMAP
new_colormap = [
    "#f0f8ff",
    #"#ecf5ff",     # celeste più chiaro
    "#d9ecff",    #new
    "#a1cff7",    #new
    "#5ca8e5",    #new
    "#2476b5",    #new
    "#0c4b78",    #new
    "#00334c",    #new
    "#005259",    #new
    "#008c69",    #new
    "#00cc44",
    "#95ff00",
    "#ffff00",
    "#ffd400",
    "#ffaa00",
    "#ff7f00",
    "#ff5500",
    "#ff2a00",
    "#f20c1f",
    "#cc1461",
    "#eb1cb7",    #new
    "#be21cc",
    "#8613bf",
    "#5f19a6",
    "#330067",    #new
]

precip_colormap = mpl.colors.ListedColormap(new_colormap)

colormap25_pr = [
    "#ecf5ff",     # celeste più chiaro
   # "#d9ecff",
   "#d9ecff",
    "#a1cff7",
    "#5ca8e5",
    "#2476b5",
    "#0c4b78",
    "#00334d",    #modified
    "#003d52",    #new
    "#005259",
    "#008c69",
    "#00cc44",
    "#95ff00",
    "#ffff00",
    "#ffd400", 
    "#ffaa00",
    "#ff7f00",
    "#ff5500",
    "#ff2a00",
    "#f20c1f",
    "#cc1461",
    "#eb1cb7",
    "#be21cc",
    "#8613bf",
    "#6419a6",    #modified
    "#450a80",    #new
    "#330066",
    ]

precip_colormap25 = mpl.colors.ListedColormap(colormap25_pr)

colormap25_pr = [
    "#ecf5ff",     # celeste più chiaro
   # "#d9ecff",
   "#d9ecff",
    "#a1cff7",
    "#5ca8e5",
    "#2476b5",
    "#0c4b78",
    "#00334d",    #modified
    "#003d52",    #new
    "#005259",
    "#008c69",
    "#00cc44",
    "#95ff00",
    "#ffff00",
    "#ffd400", 
    "#ffaa00",
    "#ff7f00",
    "#ff5500",
    "#ff2a00",
    "#f20c1f",
    "#cc1461",
    "#eb1cb7",
    "#be21cc",
    "#8613bf",
    "#6419a6",    #modified
    "#450a80",    #new
    "#330066",
    ]

precip_colormap25 = mpl.colors.ListedColormap(colormap25_pr)

#%% SOM pr
norm_factor_CERRA_LAND = 0.2301087075443887 #pr italia
#norm_factor_CERRA_LAND = 0.2629280725465797 # pr sicilia

#%% Dataset and Export location
#State the path where the file is located. This will be the same path used in MiniSOM Tutorial Step #1
PATH ="C:/Users/Criss/Documents/Lavoro/Assegno_2024_2025/Codici/SOM/new_extremes/" #This is the path where the data files
folderpath = 'C:/Users/Criss/Documents/Lavoro/Assegno_2024_2025/Codici/SOM/SOMs_output/' 

# Italy
EU_domain = "EU3"
domain_region = "Italy"

dataset_str = "CERRA_LAND"
norm_factor = norm_factor_CERRA_LAND

som_variables_str = "pr" 

print("ATTENZIONE Stai caricando i dati di "+ domain_region + " " + EU_domain + " SOM di " + som_variables_str)
data_train = np.load(PATH + dataset_str +  '_' + domain_region + '_' + som_variables_str + '_som_data_train_anomalies_' + EU_domain + '_45rm_nomask.npy')
data_test =  np.load(PATH + dataset_str+ '_' + domain_region + '_' + som_variables_str + '_som_data_test_anomalies_' + EU_domain + '_45rm_nomask.npy')
all_data = np.load(PATH + dataset_str + '_' + domain_region + '_' + som_variables_str + '_som_all_data_anomalies_' + EU_domain + '_45rm_nomask.npy')
time_values = np.load(PATH +dataset_str + '_' + domain_region + '_' + som_variables_str + '_som_time_data_anomalies_' + EU_domain + '_45rm_nomask.npy')
z_raw = xr.open_dataset(PATH + dataset_str + '_' + domain_region + '_' + som_variables_str + '_som_raw_anomalies_' + EU_domain + '_45rm_nomask.nc')

lon = z_raw['lon'].values
lat = z_raw['lat'].values
nx = int((z_raw['lat'].size))
ny = int((z_raw['lon'].size))
#ndays =int((z_raw['time'].size)) #non viene usato mai

print(data_train.shape)
len_datatrain = len(data_train[0])

time_values_test = time_values[len(data_train):]


today = date.today()

print("loading " + dataset_str + '_extreme_dates_' + domain_region + '.npy')
#extreme_dates = np.load(PATH + dataset_str + '_extreme_dates.npy', allow_pickle=True) #extremes 
#extreme_dates = np.load(PATH + dataset_str + '_extreme_dates_ITALY.npy', allow_pickle=True) #extremes 
extreme_dates = np.load(PATH + dataset_str + '_extreme_dates_' + domain_region + '.npy', allow_pickle=True) #extremes 

print(len(extreme_dates))

#%% INITIALIZE THE SOM
#You want to change these to the settings that you would like. 
som_col = 5
som_row = 5
som_shape =(som_row, som_col)
min_som = min(som_col, som_row)
#x= 3 #columns
#y= 4 #row
input_length = len_datatrain #This is value is the the length of the latitude X longitude. It is the second value in the data_train.shape step. 
sigma = min_som -1        #The sigma value must be y-1. 
#sigma = 
learning_rate = 0.0005  #Learning Rate 
qerror_list = []
q_win = 100000.
number_of_soms = 10

som_names = dataset_str + "_" + domain_region + "_" + EU_domain + "_45rm_" + som_variables_str + "_nomask_" + str(today) + "_" + str(som_col) + "by" + str(som_row) + "_LR" + str(learning_rate) + "_sig" + str(sigma) + "_n_"
print(som_names)
#%% Create the SOM
for i in range(number_of_soms):   #The number of SOMs that will be generated. 
    # initialize random weights
    era5_hourly_som1 = minisom.MiniSom(som_row, som_col, input_len = input_length, sigma = sigma, learning_rate=learning_rate, neighborhood_function='gaussian', decay_function = asymptotic_decay)
    era5_hourly_som1.random_weights_init(data_train)
    # train som
    era5_hourly_som1.train(data_train, num_iteration=100000,random_order=True, verbose=True)
    q_error = era5_hourly_som1.quantization_error(data_train)
    
    #Add the details of the SOM settings into the name of the file so that you know what the SOM is showing.
    with open(folderpath + som_names +str(i+1)+'.p', 'wb') as outfile: #this is how you save the file, the str(i) is a unique name
        pickle.dump(era5_hourly_som1, outfile)
    weights = era5_hourly_som1._weights
    qerror_list += [q_error]
    i+=1
    if q_error < q_win:
        q_win = q_error
        win_weights = era5_hourly_som1
        
print('\007')
#%%
#som_names="CERRA_LAND_EmiliaRomagna_EU2_45rm_Z500_mSLP_2024-06-07_2by2_LR0.0005_sig1_n_"
#som_names="CERRA_LAND_Italy_EU2_45rm_Z500_mSLP_2024-06-03_2by2_LR0.0005_sig1_n_"

#som_names = "CERRA_LAND_Italy_EU3_45rm_pr_nomask_2024-06-27_3by3_LR0.0005_sig2_n_"

#som_names = "CERRA_LAND_Italy_EU3_45rm_pr_nomask_2024-07-04_4by4_LR0.0005_sig3_n_"
som_names = "CERRA_LAND_Italy_EU3_45rm_pr_nomask_2024-07-04_5by5_LR0.0005_sig4_n_"
names = ([os.path.splitext(os.path.split(x)[-1])[0] for x in glob.glob(folderpath + som_names + '*')]) #this might be different for you

#but this is just grabbing the first few characters of my names of my file (see above how I named them, for example som_8)

filepaths = glob.glob(folderpath + som_names + '*')  #this is showing the path and the given file

print(names)
#%%
# new extremes
# Z500 and MSLP

path_CERRA = "C:/Users/Criss/Documents/Lavoro/Assegno_2024_2025/Dati/CERRA_LAND"  
def select_latlon(ds):
    #return ds.sel(lat = slice(60,36), lon = slice(-10,19)) #EU_1
    #return ds.sel(lat = slice(60,35), lon = slice(-15,30))#EU_2
    #return ds.sel(lat = slice(60,25), lon = slice(-13,35))#EU_3 (aggiornato)
    return ds.sel(lat = slice(36,48), lon = slice(5,19))# Italy
    #return ds.sel(lat = slice(47,25), lon = slice(-13,35))#EU_3_MEDITERRANEO -> EU4
#34N - 48M   4E - 16E
mask_file = path_CERRA + "/prova_mask.nc"
dy = xr.open_mfdataset(mask_file, preprocess=select_latlon)
#print(dy)

mask_lon = dy['lon']
mask_lat = dy['lat']
mask_nx = int((mask_lat.size))
mask_ny = int((mask_lon.size))
print(mask_nx, mask_ny)

mask = dy["stl1"].values
prova_mask = mask/mask

mask = dy["stl1"].values
prova_mask = mask/mask
pr_mask = prova_mask[0]

#%%
# we want to calculate QE and TE on both train and data test 
qerror_list_train = []
topoerror_list_train = []

qerror_list_test = []
topoerror_list_test = []
            
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        file = pickle.load(f) #This is loading every single som in that location
        frequencies = file.activation_response(data_test) #this is grabbing each freq
        q_error_train = round(file.quantization_error(data_train),3) #this is grabbing every q error out to 3 decimal places
        topo_error_train = round(file.topographic_error(data_train),3) #this is grabbing ever topographic error out to 3 decimal places
        qerror_list_train += [q_error_train]
        topoerror_list_train += [topo_error_train]
        
        q_error_test = round(file.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error_test = round(file.topographic_error(data_test),3) #this is grabbing ever topographic error out to 3 decimal places
        qerror_list_test += [q_error_test]
        topoerror_list_test += [topo_error_test]
        
mean_qerror_train = np.mean(qerror_list_train)
mean_qerror_test  = np.mean(qerror_list_test)

mean_terror_train = np.mean(topoerror_list_train)
mean_terror_test  = np.mean(topoerror_list_test)

index_best = np.where(qerror_list_test == np.min(qerror_list_test))[0][0]
print("qerr. best is " + names[index_best])

index_best = np.where(topoerror_list_test == np.min(topoerror_list_test))[0][0]
print("topoerr. best is " + names[index_best])

#%%

for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
        
        corr_list = []
        for k in range(weights.shape[0]):
            for j in range(weights.shape[1]):
                index_maps = winmap[(k,j)]
                #print(index_maps) #index of the maps of a single node
                cluster_maps = [all_data[i] for i in index_maps] # maps relative to such indices
                
                print("Node " + str(k*weights.shape[1] + j+1))
                print(" number of maps: " + str((len(cluster_maps))))
                corr_list_temp = []
                for i in range(len(cluster_maps)):
                    corr_list_temp += [np.corrcoef(weights[k,j,:], cluster_maps[i])[0,1]]
            
                print(" corr.: " + str(np.mean(corr_list_temp))) 
                corr_list += [np.mean(corr_list_temp)]
              
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        q_error = round(som.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error = round(som.topographic_error(data_test),3) 
        
        #Sammonplot
        """
        frame = plt.figure(figsize=(8,7))
        cs = plt.pcolormesh(freq_perc, cmap="Blues")
   
        plt.title('SOM Freq.: ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:] + ' QE =' + ' ' f"{q_error}" + ' ' + 'TE =' + ' ' f"{topo_error}", fontsize=12)

        #in the title, I am plotting every q error and topo error from each som. You need to have the f" in front and whatever variable in {}
        #And this ' ' represents a space in the title
        plt.colorbar(cs, label="freq [%]")
        plt.ylim(som_row,0) # Change the 3 to whatever size SOM you have (this is the 2nd number)
        plt.tight_layout()
        plt.xticks(ticks=[])
        plt.yticks(ticks=[])
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'_freq.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM
        plt.show()
        """
        
        
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 25),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
    
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(nx,ny)
                
                SOM_z500 = SOM_array/ norm_factor # è pr
                
                # THIS IS FOR Z500 MSLP SOMS
                #levs = [ 0, 1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]
                #n_cmap = len(levs) # TYPE 1
                #norm = mpl.colors.BoundaryNorm(levs, n_cmap)
                #cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_z500*pr_mask,
                #                  transform = ccrs.PlateCarree(),
                #                  cmap=precip_colormap, norm=norm, levels = levs,extend='both')
                levs = np.arange(0, 105, 5)
                n_cmap = len(levs)
                norm = mpl.colors.BoundaryNorm(levs, n_cmap)
                cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_z500*pr_mask,
                                  transform = ccrs.PlateCarree(),
                                  cmap=precip_colormap, levels = levs,extend='both')
                label_shaded = "pr (mm)"
                
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)


                # Title each subplot 
                axs[(k*som_col)+i].set_title("F:" + str(int(frequencies[k,i])) + " (" + "%.2f" % freq_perc[k,i] + " %) " + r"$\rho$: %.2f" % corr_list[node] , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, norm=norm, ticks = levs,orientation='horizontal') # TYPE1
        cbar=fig.colorbar(cs2,cax=cbar_ax, norm=norm, ticks = levs,orientation='horizontal')
        
        cbar.set_label(label_shaded, fontsize=22)
        #cbar.set_label(r"anom. TCWV ($kg / m^2$)", fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'.png', bbox_inches='tight')
        plt.show()

#%% SOM Z500-pr
norm_factor_CERRA_LAND = 0.098014048959525 #pr italia

# Italy
EU_domain = "EU3"
domain_region = "Italy"

dataset_str = "CERRA_LAND"
norm_factor = norm_factor_CERRA_LAND

som_variables_str = "Z500_pr" 

print("ATTENZIONE Stai caricando i dati di "+ domain_region + " " + EU_domain + " SOM di " + som_variables_str)
data_train = np.load(PATH + dataset_str +  '_' + domain_region + '_' + som_variables_str + '_som_data_train_anomalies_' + EU_domain + '_45rm.npy')
data_test =  np.load(PATH + dataset_str+ '_' + domain_region + '_' + som_variables_str + '_som_data_test_anomalies_' + EU_domain + '_45rm.npy')
all_data = np.load(PATH + dataset_str + '_' + domain_region + '_' + som_variables_str + '_som_all_data_anomalies_' + EU_domain + '_45rm.npy')
time_values = np.load(PATH +dataset_str + '_' + domain_region + '_' + som_variables_str + '_som_time_data_anomalies_' + EU_domain + '_45rm.npy')
z_raw = xr.open_dataset(PATH + dataset_str + '_' + domain_region + '_' + som_variables_str + '_Z500_raw_' + EU_domain + '_45rm.nc')
pr_raw = xr.open_dataset(PATH + dataset_str + '_' + domain_region + '_' + som_variables_str + '_pr_raw_' + EU_domain + '_45rm.nc')

z_lon = z_raw['lon'].values
z_lat = z_raw['lat'].values
z_nx = int((z_raw['lat'].size))
z_ny = int((z_raw['lon'].size))

pr_lon = pr_raw['lon'].values
pr_lat = pr_raw['lat'].values
pr_nx = int((pr_raw['lat'].size))
pr_ny = int((pr_raw['lon'].size))

print(z_nx, z_ny)
print(pr_nx, pr_ny)
#ndays =int((z_raw['time'].size)) #non viene usato mai

print(data_train.shape)
len_datatrain = len(data_train[0])

time_values_test = time_values[len(data_train):]


today = date.today()

print("loading " + dataset_str + '_extreme_dates_' + domain_region + '.npy')
#extreme_dates = np.load(PATH + dataset_str + '_extreme_dates.npy', allow_pickle=True) #extremes 
#extreme_dates = np.load(PATH + dataset_str + '_extreme_dates_ITALY.npy', allow_pickle=True) #extremes 
extreme_dates = np.load(PATH + dataset_str + '_extreme_dates_' + domain_region + '.npy', allow_pickle=True) #extremes 

print(len(extreme_dates))

#%% INITIALIZE THE SOM
#You want to change these to the settings that you would like. 
som_col = 3
som_row = 3
som_shape =(som_row, som_col)
min_som = min(som_col, som_row)
#x= 3 #columns
#y= 4 #row
input_length = len_datatrain #This is value is the the length of the latitude X longitude. It is the second value in the data_train.shape step. 
sigma = min_som -1        #The sigma value must be y-1. 
#sigma = 
learning_rate = 0.0005  #Learning Rate 
qerror_list = []
q_win = 100000.
number_of_soms = 10

som_names = dataset_str + "_" + domain_region + "_" + EU_domain + "_45rm_" + som_variables_str + "_nomask_" + str(today) + "_" + str(som_col) + "by" + str(som_row) + "_LR" + str(learning_rate) + "_sig" + str(sigma) + "_n_"
print(som_names)
#%% Create the SOM
for i in range(number_of_soms):   #The number of SOMs that will be generated. 
    # initialize random weights
    era5_hourly_som1 = minisom.MiniSom(som_row, som_col, input_len = input_length, sigma = sigma, learning_rate=learning_rate, neighborhood_function='gaussian', decay_function = asymptotic_decay)
    era5_hourly_som1.random_weights_init(data_train)
    # train som
    era5_hourly_som1.train(data_train, num_iteration=100000,random_order=True, verbose=True)
    q_error = era5_hourly_som1.quantization_error(data_train)
    
    #Add the details of the SOM settings into the name of the file so that you know what the SOM is showing.
    with open(folderpath + som_names +str(i+1)+'.p', 'wb') as outfile: #this is how you save the file, the str(i) is a unique name
        pickle.dump(era5_hourly_som1, outfile)
    weights = era5_hourly_som1._weights
    qerror_list += [q_error]
    i+=1
    if q_error < q_win:
        q_win = q_error
        win_weights = era5_hourly_som1
        
print('\007')
#%%
#som_names="CERRA_LAND_EmiliaRomagna_EU2_45rm_Z500_mSLP_2024-06-07_2by2_LR0.0005_sig1_n_"
#som_names="CERRA_LAND_Italy_EU2_45rm_Z500_mSLP_2024-06-03_2by2_LR0.0005_sig1_n_"

#som_names = "CERRA_LAND_Italy_EU3_45rm_pr_nomask_2024-06-27"+ "_" + str(som_col) + "by" + str(som_row) + "_LR" + str(learning_rate) + "_sig" + str(sigma) + "_n_"

som_names = "CERRA_LAND_Italy_EU3_45rm_Z500_pr_nomask_2024-07-04" + "_" + str(som_col) + "by" + str(som_row) + "_LR" + str(learning_rate) + "_sig" + str(sigma) + "_n_"
names = ([os.path.splitext(os.path.split(x)[-1])[0] for x in glob.glob(folderpath + som_names + '*')]) #this might be different for you

#but this is just grabbing the first few characters of my names of my file (see above how I named them, for example som_8)

filepaths = glob.glob(folderpath + som_names + '*')  #this is showing the path and the given file

print(names)

#%%
# we want to calculate QE and TE on both train and data test 
qerror_list_train = []
topoerror_list_train = []

qerror_list_test = []
topoerror_list_test = []
            
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        file = pickle.load(f) #This is loading every single som in that location
        frequencies = file.activation_response(data_test) #this is grabbing each freq
        q_error_train = round(file.quantization_error(data_train),3) #this is grabbing every q error out to 3 decimal places
        topo_error_train = round(file.topographic_error(data_train),3) #this is grabbing ever topographic error out to 3 decimal places
        qerror_list_train += [q_error_train]
        topoerror_list_train += [topo_error_train]
        
        q_error_test = round(file.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error_test = round(file.topographic_error(data_test),3) #this is grabbing ever topographic error out to 3 decimal places
        qerror_list_test += [q_error_test]
        topoerror_list_test += [topo_error_test]
        
mean_qerror_train = np.mean(qerror_list_train)
mean_qerror_test  = np.mean(qerror_list_test)

mean_terror_train = np.mean(topoerror_list_train)
mean_terror_test  = np.mean(topoerror_list_test)

index_best = np.where(qerror_list_test == np.min(qerror_list_test))[0][0]
print("qerr. best is " + names[index_best])

index_best = np.where(topoerror_list_test == np.min(topoerror_list_test))[0][0]
print("topoerr. best is " + names[index_best])

#%%
path_CERRA = "C:/Users/Criss/Documents/Lavoro/Assegno_2024_2025/Dati/CERRA_LAND"  
def select_latlon(ds):
    #return ds.sel(lat = slice(60,36), lon = slice(-10,19)) #EU_1
    #return ds.sel(lat = slice(60,35), lon = slice(-15,30))#EU_2
    #return ds.sel(lat = slice(60,25), lon = slice(-13,35))#EU_3 (aggiornato)
    return ds.sel(lat = slice(36,48), lon = slice(5,19))# Italy
    #return ds.sel(lat = slice(47,25), lon = slice(-13,35))#EU_3_MEDITERRANEO -> EU4
#34N - 48M   4E - 16E
mask_file = path_CERRA + "/prova_mask.nc"
dy = xr.open_mfdataset(mask_file, preprocess=select_latlon)
print(dy)

mask_lon = dy['lon']
mask_lat = dy['lat']
mask_nx = int((mask_lat.size))
mask_ny = int((mask_lon.size))
print(mask_nx, mask_ny)

mask = dy["stl1"].values
prova_mask = mask/mask

mask = dy["stl1"].values
prova_mask = mask/mask
prova_mask = prova_mask[0]

#%%
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
            
        corr_list = []
        for k in range(weights.shape[0]):
            for j in range(weights.shape[1]):
                index_maps = winmap[(k,j)]
                #print(index_maps) #index of the maps of a single node
                cluster_maps = [all_data[i] for i in index_maps] # maps relative to such indices
                
                print("Node " + str(k*weights.shape[1] + j+1))
                print(" number of maps: " + str((len(cluster_maps))))
                corr_list_temp = []
                for i in range(len(cluster_maps)):
                    corr_list_temp += [np.corrcoef(weights[k,j,:], cluster_maps[i])[0,1]]
            
                print(" corr.: " + str(np.mean(corr_list_temp))) 
                corr_list += [np.mean(corr_list_temp)]
                
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        
        q_error = round(som.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error = round(som.topographic_error(data_test),3) 
        
        """
        frame = plt.figure(figsize=(8,7))
        cs = plt.pcolormesh(freq_perc, cmap="Blues")
   
        plt.title('SOM Freq.: ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:] + ' QE =' + ' ' f"{q_error}" + ' ' + 'TE =' + ' ' f"{topo_error}", fontsize=12)

        #in the title, I am plotting every q error and topo error from each som. You need to have the f" in front and whatever variable in {}
        #And this ' ' represents a space in the title
        plt.colorbar(cs, label="freq [%]")
        plt.ylim(som_row,0) # Change the 3 to whatever size SOM you have (this is the 2nd number)
        plt.tight_layout()
        plt.xticks(ticks=[])
        plt.yticks(ticks=[])
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'_freq.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM
        plt.show()
        """
        
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 20),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
        
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(z_nx,z_ny,2)
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_pr = SOM_array[:pr_nx,:pr_ny,1]  / norm_factor 
                
                # pr
                levs = np.arange(0, 105, 5)
               
                cs2=axs[(k*som_col)+i].contourf(pr_lon, pr_lat, SOM_pr*prova_mask[0],
                                  transform = ccrs.PlateCarree(),
                                  cmap=precip_colormap, levels = levs,extend='both')
                label_shaded = "pr (mm)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(z_lon, z_lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)

                axs[(k*som_col)+i].set_extent([-13, 35, 60, 25], ccrs.PlateCarree())
                # Title each subplot 
                axs[(k*som_col)+i].set_title("F:" + str(int(frequencies[k,i])) + " (" + "%.2f" % freq_perc[k,i] + " %) " + r"$\rho$: %.2f" % corr_list[node] , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'.png', bbox_inches='tight')
        plt.show()
        

#%%
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
            
        corr_list = []
        for k in range(weights.shape[0]):
            for j in range(weights.shape[1]):
                index_maps = winmap[(k,j)]
                #print(index_maps) #index of the maps of a single node
                cluster_maps = [all_data[i] for i in index_maps] # maps relative to such indices
                
                print("Node " + str(k*weights.shape[1] + j+1))
                print(" number of maps: " + str((len(cluster_maps))))
                corr_list_temp = []
                for i in range(len(cluster_maps)):
                    corr_list_temp += [np.corrcoef(weights[k,j,:], cluster_maps[i])[0,1]]
            
                print(" corr.: " + str(np.mean(corr_list_temp))) 
                corr_list += [np.mean(corr_list_temp)]
                
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 20),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
        
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(z_nx,z_ny,2)
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_pr = SOM_array[:pr_nx,:pr_ny,1]  / norm_factor 
                
                # pr
                levs = np.arange(0, 105, 5)
                cs2=axs[(k*som_col)+i].contourf(pr_lon, pr_lat, SOM_pr*prova_mask[0],
                                  transform = ccrs.PlateCarree(),
                                  cmap=precip_colormap, levels = levs,extend='both')
                label_shaded = "pr (mm)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(z_lon, z_lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)
                axs[(k*som_col)+i].set_extent([5, 19, 48, 36], ccrs.PlateCarree())

                # Title each subplot 
                axs[(k*som_col)+i].set_title("F:" + str(int(frequencies[k,i])) + " (" + "%.2f" % freq_perc[k,i] + " %) " + r"$\rho$: %.2f" % corr_list[node] , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'_IT.png', bbox_inches='tight')
        plt.show()
        
#%% TEMP

som_col = 4
som_row = 4
som_shape =(som_row, som_col)
min_som = min(som_col, som_row)
#x= 3 #columns
#y= 4 #row
input_length = len_datatrain #This is value is the the length of the latitude X longitude. It is the second value in the data_train.shape step. 
sigma = min_som -1        #The sigma value must be y-1. 
#sigma = 
learning_rate = 0.0005  #Learning Rate 
qerror_list = []
q_win = 100000.
number_of_soms = 10


som_names = "CERRA_LAND_Italy_EU3_45rm_Z500_pr_nomask_2024-07-04" + "_" + str(som_col) + "by" + str(som_row) + "_LR" + str(learning_rate) + "_sig" + str(sigma) + "_n_"
names = ([os.path.splitext(os.path.split(x)[-1])[0] for x in glob.glob(folderpath + som_names + '*')]) #this might be different for you

#but this is just grabbing the first few characters of my names of my file (see above how I named them, for example som_8)

filepaths = glob.glob(folderpath + som_names + '*')  #this is showing the path and the given file

print(names)

for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
            
        corr_list = []
        for k in range(weights.shape[0]):
            for j in range(weights.shape[1]):
                index_maps = winmap[(k,j)]
                #print(index_maps) #index of the maps of a single node
                cluster_maps = [all_data[i] for i in index_maps] # maps relative to such indices
                
                print("Node " + str(k*weights.shape[1] + j+1))
                print(" number of maps: " + str((len(cluster_maps))))
                corr_list_temp = []
                for i in range(len(cluster_maps)):
                    corr_list_temp += [np.corrcoef(weights[k,j,:], cluster_maps[i])[0,1]]
            
                print(" corr.: " + str(np.mean(corr_list_temp))) 
                corr_list += [np.mean(corr_list_temp)]
                
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        
        q_error = round(som.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error = round(som.topographic_error(data_test),3) 
        
        """
        frame = plt.figure(figsize=(8,7))
        cs = plt.pcolormesh(freq_perc, cmap="Blues")
   
        plt.title('SOM Freq.: ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:] + ' QE =' + ' ' f"{q_error}" + ' ' + 'TE =' + ' ' f"{topo_error}", fontsize=12)

        #in the title, I am plotting every q error and topo error from each som. You need to have the f" in front and whatever variable in {}
        #And this ' ' represents a space in the title
        plt.colorbar(cs, label="freq [%]")
        plt.ylim(som_row,0) # Change the 3 to whatever size SOM you have (this is the 2nd number)
        plt.tight_layout()
        plt.xticks(ticks=[])
        plt.yticks(ticks=[])
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'_freq.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM
        plt.show()
        """
        
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 20),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
        
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(z_nx,z_ny,2)
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_pr = SOM_array[:pr_nx,:pr_ny,1]  / norm_factor 
                
                # pr
                levs = np.arange(0, 105, 5)
               
                cs2=axs[(k*som_col)+i].contourf(pr_lon, pr_lat, SOM_pr*prova_mask[0],
                                  transform = ccrs.PlateCarree(),
                                  cmap=precip_colormap, levels = levs,extend='both')
                label_shaded = "pr (mm)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(z_lon, z_lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)

                axs[(k*som_col)+i].set_extent([-13, 35, 60, 25], ccrs.PlateCarree())
                # Title each subplot 
                axs[(k*som_col)+i].set_title("F:" + str(int(frequencies[k,i])) + " (" + "%.2f" % freq_perc[k,i] + " %) " + r"$\rho$: %.2f" % corr_list[node] , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'.png', bbox_inches='tight')
        plt.show()

for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
            
        corr_list = []
        for k in range(weights.shape[0]):
            for j in range(weights.shape[1]):
                index_maps = winmap[(k,j)]
                #print(index_maps) #index of the maps of a single node
                cluster_maps = [all_data[i] for i in index_maps] # maps relative to such indices
                
                print("Node " + str(k*weights.shape[1] + j+1))
                print(" number of maps: " + str((len(cluster_maps))))
                corr_list_temp = []
                for i in range(len(cluster_maps)):
                    corr_list_temp += [np.corrcoef(weights[k,j,:], cluster_maps[i])[0,1]]
            
                print(" corr.: " + str(np.mean(corr_list_temp))) 
                corr_list += [np.mean(corr_list_temp)]
                
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 20),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
        
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(z_nx,z_ny,2)
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_pr = SOM_array[:pr_nx,:pr_ny,1]  / norm_factor 
                
                # pr
                levs = np.arange(0, 105, 5)
                cs2=axs[(k*som_col)+i].contourf(pr_lon, pr_lat, SOM_pr*prova_mask[0],
                                  transform = ccrs.PlateCarree(),
                                  cmap=precip_colormap, levels = levs,extend='both')
                label_shaded = "pr (mm)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(z_lon, z_lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)
                axs[(k*som_col)+i].set_extent([5, 19, 48, 36], ccrs.PlateCarree())

                # Title each subplot 
                axs[(k*som_col)+i].set_title("F:" + str(int(frequencies[k,i])) + " (" + "%.2f" % freq_perc[k,i] + " %) " + r"$\rho$: %.2f" % corr_list[node] , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'_IT.png', bbox_inches='tight')
        plt.show()
        
#%%
som_col = 5
som_row = 5
som_shape =(som_row, som_col)
min_som = min(som_col, som_row)
#x= 3 #columns
#y= 4 #row
input_length = len_datatrain #This is value is the the length of the latitude X longitude. It is the second value in the data_train.shape step. 
sigma = min_som -1        #The sigma value must be y-1. 
#sigma = 
learning_rate = 0.0005  #Learning Rate 
qerror_list = []
q_win = 100000.
number_of_soms = 10


som_names = "CERRA_LAND_Italy_EU3_45rm_Z500_pr_nomask_2024-07-04" + "_" + str(som_col) + "by" + str(som_row) + "_LR" + str(learning_rate) + "_sig" + str(sigma) + "_n_"
names = ([os.path.splitext(os.path.split(x)[-1])[0] for x in glob.glob(folderpath + som_names + '*')]) #this might be different for you

#but this is just grabbing the first few characters of my names of my file (see above how I named them, for example som_8)

filepaths = glob.glob(folderpath + som_names + '*')  #this is showing the path and the given file

print(names)

for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
            
        corr_list = []
        for k in range(weights.shape[0]):
            for j in range(weights.shape[1]):
                index_maps = winmap[(k,j)]
                #print(index_maps) #index of the maps of a single node
                cluster_maps = [all_data[i] for i in index_maps] # maps relative to such indices
                
                print("Node " + str(k*weights.shape[1] + j+1))
                print(" number of maps: " + str((len(cluster_maps))))
                corr_list_temp = []
                for i in range(len(cluster_maps)):
                    corr_list_temp += [np.corrcoef(weights[k,j,:], cluster_maps[i])[0,1]]
            
                print(" corr.: " + str(np.mean(corr_list_temp))) 
                corr_list += [np.mean(corr_list_temp)]
                
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        
        q_error = round(som.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error = round(som.topographic_error(data_test),3) 
        
        """
        frame = plt.figure(figsize=(8,7))
        cs = plt.pcolormesh(freq_perc, cmap="Blues")
   
        plt.title('SOM Freq.: ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:] + ' QE =' + ' ' f"{q_error}" + ' ' + 'TE =' + ' ' f"{topo_error}", fontsize=12)

        #in the title, I am plotting every q error and topo error from each som. You need to have the f" in front and whatever variable in {}
        #And this ' ' represents a space in the title
        plt.colorbar(cs, label="freq [%]")
        plt.ylim(som_row,0) # Change the 3 to whatever size SOM you have (this is the 2nd number)
        plt.tight_layout()
        plt.xticks(ticks=[])
        plt.yticks(ticks=[])
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'_freq.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM
        plt.show()
        """
        
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 20),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
        
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(z_nx,z_ny,2)
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_pr = SOM_array[:pr_nx,:pr_ny,1]  / norm_factor 
                
                # pr
                levs = np.arange(0, 105, 5)
               
                cs2=axs[(k*som_col)+i].contourf(pr_lon, pr_lat, SOM_pr*prova_mask[0],
                                  transform = ccrs.PlateCarree(),
                                  cmap=precip_colormap, levels = levs,extend='both')
                label_shaded = "pr (mm)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(z_lon, z_lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)

                axs[(k*som_col)+i].set_extent([-13, 35, 60, 25], ccrs.PlateCarree())
                # Title each subplot 
                axs[(k*som_col)+i].set_title("F:" + str(int(frequencies[k,i])) + " (" + "%.2f" % freq_perc[k,i] + " %) " + r"$\rho$: %.2f" % corr_list[node] , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'.png', bbox_inches='tight')
        plt.show()

for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
            
        corr_list = []
        for k in range(weights.shape[0]):
            for j in range(weights.shape[1]):
                index_maps = winmap[(k,j)]
                #print(index_maps) #index of the maps of a single node
                cluster_maps = [all_data[i] for i in index_maps] # maps relative to such indices
                
                print("Node " + str(k*weights.shape[1] + j+1))
                print(" number of maps: " + str((len(cluster_maps))))
                corr_list_temp = []
                for i in range(len(cluster_maps)):
                    corr_list_temp += [np.corrcoef(weights[k,j,:], cluster_maps[i])[0,1]]
            
                print(" corr.: " + str(np.mean(corr_list_temp))) 
                corr_list += [np.mean(corr_list_temp)]
                
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 20),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
        
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(z_nx,z_ny,2)
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_pr = SOM_array[:pr_nx,:pr_ny,1]  / norm_factor 
                
                # pr
                levs = np.arange(0, 105, 5)
                cs2=axs[(k*som_col)+i].contourf(pr_lon, pr_lat, SOM_pr*prova_mask[0],
                                  transform = ccrs.PlateCarree(),
                                  cmap=precip_colormap, levels = levs,extend='both')
                label_shaded = "pr (mm)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(z_lon, z_lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)
                axs[(k*som_col)+i].set_extent([5, 19, 48, 36], ccrs.PlateCarree())

                # Title each subplot 
                axs[(k*som_col)+i].set_title("F:" + str(int(frequencies[k,i])) + " (" + "%.2f" % freq_perc[k,i] + " %) " + r"$\rho$: %.2f" % corr_list[node] , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        plt.savefig(folderpath + dataset_str + '_all_node_' +name+'_IT.png', bbox_inches='tight')
        plt.show()