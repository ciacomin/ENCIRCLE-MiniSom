# -*- coding: utf-8 -*-
"""
Created on Wed Apr 17 12:38:04 2024

@author: Criss
"""

import os

os.chdir('C:\\Users\\Criss\\Documents\\Lavoro\\Assegno_2024_2025\\Codici')

#Imports 
import xarray as xr
import numpy as np
import pandas as pd
import warnings
import minisom
import pickle
from minisom import asymptotic_decay
import cartopy.mpl.ticker as cticker
from cartopy.util import add_cyclic_point
from mpl_toolkits.mplot3d import Axes3D
import matplotlib as mpl
from numpy import savetxt
from numpy import loadtxt
import cartopy
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import glob
import matplotlib.pyplot as plt
import os
from itertools import product
import winsound
from datetime import date

#Functions Used in the Code
def getList(dict):
    list = []
    for key in dict.keys():
        list.append(key)
        
    return list
def sammon(x, n, display = 2, inputdist = 'raw', maxhalves = 20, maxiter = 500, tolfun = 1e-9, init = 'default'):

    import numpy as np 
    from scipy.spatial.distance import cdist

    """Perform Sammon mapping on dataset x
    y = sammon(x) applies the Sammon nonlinear mapping procedure on
    multivariate data x, where each row represents a pattern and each column
    represents a feature.  On completion, y contains the corresponding
    co-ordinates of each point on the map.  By default, a two-dimensional
    map is created.  Note if x contains any duplicated rows, SAMMON will
    fail (ungracefully). 
    [y,E] = sammon(x) also returns the value of the cost function in E (i.e.
    the stress of the mapping).
    An N-dimensional output map is generated by y = sammon(x,n) .
    A set of optimisation options can be specified using optional
    arguments, y = sammon(x,n,[OPTS]):
       maxiter        - maximum number of iterations
       tolfun         - relative tolerance on objective function
       maxhalves      - maximum number of step halvings
       input          - {'raw','distance'} if set to 'distance', X is 
                        interpreted as a matrix of pairwise distances.
       display        - 0 to 2. 0 least verbose, 2 max verbose.
       init           - {'pca', 'cmdscale', random', 'default'}
                        default is 'pca' if input is 'raw', 
                        'msdcale' if input is 'distance'
    The default options are retrieved by calling sammon(x) with no
    parameters.
    File        : sammon.py
    Date        : 18 April 2014
    Authors     : Tom J. Pollard (tom.pollard.11@ucl.ac.uk)
                : Ported from MATLAB implementation by 
                  Gavin C. Cawley and Nicola L. C. Talbot
    Description : Simple python implementation of Sammon's non-linear
                  mapping algorithm [1].
    References  : [1] Sammon, John W. Jr., "A Nonlinear Mapping for Data
                  Structure Analysis", IEEE Transactions on Computers,
                  vol. C-18, no. 5, pp 401-409, May 1969.
    Copyright   : (c) Dr Gavin C. Cawley, November 2007.
    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program; if not, write to the Free Software
    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
    """

    # Create distance matrix unless given by parameters
    if inputdist == 'distance':
        D = x
        if init == 'default':
            init = 'cmdscale'
    else:
        D = cdist(x, x)
        if init == 'default':
            init = 'pca'

    if inputdist == 'distance' and init == 'pca':
        raise ValueError("Cannot use init == 'pca' when inputdist == 'distance'")

    if np.count_nonzero(np.diagonal(D)) > 0:
        raise ValueError("The diagonal of the dissimilarity matrix must be zero")

    # Remaining initialisation
    N = x.shape[0]
    scale = 0.5 / D.sum()
    D = D + np.eye(N)     
    if np.count_nonzero(D<=0) > 0:
        raise ValueError("Off-diagonal dissimilarities must be strictly positive")   

    Dinv = 1 / D
    if init == 'pca':
        [UU,DD,_] = np.linalg.svd(x)
        y = UU[:,:n]*DD[:n] 
    elif init == 'cmdscale':
        from cmdscale import cmdscale
        y,e = cmdscale(D)
        y = y[:,:n]
    else:
        y = np.random.normal(0.0,1.0,[N,n])
    one = np.ones([N,n])
    d = cdist(y,y) + np.eye(N)
    dinv = 1. / d
    delta = D-d 
    E = ((delta**2)*Dinv).sum() 

    # Get on with it
    for i in range(maxiter):

        # Compute gradient, Hessian and search direction (note it is actually
        # 1/4 of the gradient and Hessian, but the step size is just the ratio
        # of the gradient and the diagonal of the Hessian so it doesn't
        # matter).
        delta = dinv - Dinv
        deltaone = np.dot(delta,one)
        g = np.dot(delta,y) - (y * deltaone)
        dinv3 = dinv ** 3
        y2 = y ** 2
        H = np.dot(dinv3,y2) - deltaone - np.dot(2,y) * np.dot(dinv3,y) + y2 * np.dot(dinv3,one)
        s = -g.flatten(order='F') / np.abs(H.flatten(order='F'))
        y_old    = y

        # Use step-halving procedure to ensure progress is made
        for j in range(maxhalves):
            s_reshape = np.reshape(s, (-1,n),order='F')
            y = y_old + s_reshape
            d = cdist(y, y) + np.eye(N)
            dinv = 1 / d
            delta = D - d
            E_new = ((delta**2)*Dinv).sum()
            if E_new < E:
                break
            else:
                s = 0.5*s

        # Bomb out if too many halving steps are required
        if j == maxhalves-1:
            print('Warning: maxhalves exceeded. Sammon mapping may not converge...')

        # Evaluate termination criterion
        if abs((E - E_new) / E) < tolfun:
            if display:
                print('TolFun exceeded: Optimisation terminated')
            break

        # Report progress
        E = E_new
        if display > 1:
            print('epoch = %d : E = %12.10f'% (i+1, E * scale))

    if i == maxiter-1:
        print('Warning: maxiter exceeded. Sammon mapping may not have converged...')

    # Fiddle stress to match the original Sammon paper
    E = E * scale
    
    return [y,E]

#my functions 

def divide_dates_by_index(list_of_dates, list_of_indices, number_indices):
    # Initialize a list of empty lists
    divided_dates = [[] for _ in range(number_indices)]
    
    # Iterate through dates and indices
    for date, index in zip(list_of_dates, list_of_indices):
        # Add the date to the list corresponding to the index
        divided_dates[index].append(date)
    
    return divided_dates

# COLORMAP
new_colormap = [
    "#f0f8ff",
    #"#ecf5ff",     # celeste più chiaro
    "#d9ecff",    #new
    "#a1cff7",    #new
    "#5ca8e5",    #new
    "#2476b5",    #new
    "#0c4b78",    #new
    "#00334c",    #new
    "#005259",    #new
    "#008c69",    #new
    "#00cc44",
    "#95ff00",
    "#ffff00",
    "#ffd400",
    "#ffaa00",
    "#ff7f00",
    "#ff5500",
    "#ff2a00",
    "#f20c1f",
    "#cc1461",
    "#eb1cb7",    #new
    "#be21cc",
    "#8613bf",
    "#5f19a6",
    "#330067",    #new
]

precip_colormap = mpl.colors.ListedColormap(new_colormap)

colormap25_pr = [
    "#ecf5ff",     # celeste più chiaro
   # "#d9ecff",
   "#d9ecff",
    "#a1cff7",
    "#5ca8e5",
    "#2476b5",
    "#0c4b78",
    "#00334d",    #modified
    "#003d52",    #new
    "#005259",
    "#008c69",
    "#00cc44",
    "#95ff00",
    "#ffff00",
    "#ffd400", 
    "#ffaa00",
    "#ff7f00",
    "#ff5500",
    "#ff2a00",
    "#f20c1f",
    "#cc1461",
    "#eb1cb7",
    "#be21cc",
    "#8613bf",
    "#6419a6",    #modified
    "#450a80",    #new
    "#330066",
    ]

precip_colormap25 = mpl.colors.ListedColormap(colormap25_pr)

colormap25_stag = [
    "#a1cff7",
    "#5ca8e5",
    "#2476b5",
    "#0c4b78",
    "#012d57",    #modified
    "#0e08bd",
    "#003d52",    #new
    "#005259",
    "#008c69",
    "#00cc44",
    "#95ff00",
    "#ffff00",
    "#ffd400", 
    "#ffaa00",
    "#ff7f00",
    "#ff5500",
    "#ff2a00",
    "#f20c1f",
    "#cc1461",
    "#eb1cb7",
    "#be21cc",
    "#8613bf",
    "#6419a6",    #modified
    "#450a80",    #new
    "#330066",
    "#808080", "#000000"
    ]

stag_colormap25 = mpl.colors.ListedColormap(colormap25_stag)
#from previous file
#z500_factor = 0.1041881651999255 # EU1 EU2, 1clim
#z500_factor = 0.10300393136281732 # EU2, 45clim
#mslp_factor = 1.034371024100864 #EU1 1clim
#mslp_factor = 1.0304879278416301 #EU2 1clim
#mslp_factor = 1.0131736067780721 #EU2 45clim

#z500_mslp_factor = 0.10300393136281732
#z500_tcwv_factor = 0.09240839955352485
#%%
"""
Remember to select the right one
"""
# new extremes Z500 MSLP
# ARCIS
norm_factor_arcis = 0.10637970464308713 # Z500 MSLP
# MSWEP
norm_factor_MSWEP = 0.09685464768116944 # Z500 MSLP
# CERRA - LAND
norm_factor_CERRA_LAND = 0.098014048959525 # Z500 MSLP

# new extremes Z500 TCWV
# ARCIS
norm_factor_arcis = 0.10637970464308713 # Z500 TCWV
# MSWEP
norm_factor_MSWEP = 0.09685464768116944 # Z500 TCWV
# CERRA - LAND
norm_factor_CERRA_LAND = 0.098014048959525 # Z500 TCWV

#%% ITALY
#norm_factor_CERRA_LAND = 0.09698036159516121 # Z500 MSLP EU2 penso
#norm_factor_CERRA_LAND = 0.09698036159516121 # Z500 TCWV
# EU3 è uguale           0.09698036159516121
# EU3 dominio aggiornato
norm_factor_CERRA_LAND = 0.098014048959525 # Z500 MSLP

# i think the norm factors are the same because Z500 has greater anomalies than MSLP and TCWV
#%% Regions
norm_factor_CERRA_LAND = 0.11166314913661772 #Sicily
norm_factor_CERRA_LAND = 0.11166314913661772 #Sicily EU3
norm_factor_CERRA_LAND = 0.1260411336901473 # EU3 restrained at lat = slice(60,35), lon = slice(-15,30))
#norm_factor_CERRA_LAND = 0.14794403482319404 # EU3 restrained at (lat = slice(47,25), lon = slice(-13,35))#EU_3_MEDITERRANEO -> EU4
#norm_factor_CERRA_LAND = 0.1277725150121397 #Italy EU4
norm_factor_CERRA_LAND = 0.10933391594928518 #Emilia Romagna 
#%% Dataset and Export location
#State the path where the file is located. This will be the same path used in MiniSOM Tutorial Step #1
PATH ="C:/Users/Criss/Documents/Lavoro/Assegno_2024_2025/Codici/SOM/new_extremes/" #This is the path where the data files
folderpath = 'C:/Users/Criss/Documents/Lavoro/Assegno_2024_2025/Codici/SOM/SOMs_output/'  #The output SOMs will be stored in a seperate folder. This will need to be changed to the User's specific path and folder name
# Z500
"""
data_train = np.load(PATH + 'Z500_som_data_train_All_CAT_anomalies_EU2_45rm.npy')
data_test =  np.load(PATH + 'Z500_som_data_test_All_CAT_anomalies_EU2_45rm.npy')
all_data = np.load(PATH + 'Z500_som_all_data_All_CAT_anomalies_EU2_45rm.npy')
time_values = np.load(PATH +'Z500_som_time_data_All_CAT_anomalies_EU2_45rm.npy')
z_raw = xr.open_dataset(PATH + 'Z500_som_raw_All_CAT_anomalies_EU2_45rm.nc')
#z_mean_array = np.load(PATH +'Z500_mean_array_All_CAT_anomalies.npy')
norm_factor = z500_factor 
str_variable = "Z500"
som_str_variable = "Z500"
unit_variable = "m"
"""

# MSLP
"""
data_train = np.load(PATH + 'MSLP_som_data_train_All_CAT_anomalies_EU2.npy')
data_test =  np.load(PATH + 'MSLP_som_data_test_All_CAT_anomalies_EU2.npy')
all_data = np.load(PATH + 'MSLP_som_all_data_All_CAT_anomalies_EU2.npy')
time_values = np.load(PATH +'MSLP_som_time_data_All_CAT_anomalies_EU2.npy') 
z_raw = xr.open_dataset(PATH + 'MSLP_som_raw_All_CAT_anomalies_EU2.nc')
#z_mean_array = np.load(PATH +'MSLP_mean_array_All_CAT_anomalies.npy')
norm_factor = mslp_factor
str_variable = "mSLP"
som_str_variable = "mSLP"
unit_variable = "hPa"
"""
"""
# Z500 and MSLP
data_train = np.load(PATH + 'Z500_MSLP_som_data_train_All_CAT_anomalies_EU2_45rm.npy')
data_test =  np.load(PATH + 'Z500_MSLP_som_data_test_All_CAT_anomalies_EU2_45rm.npy')
all_data = np.load(PATH + 'Z500_MSLP_som_all_data_All_CAT_anomalies_EU2_45rm.npy')
time_values = np.load(PATH +'Z500_MSLP_som_time_data_All_CAT_anomalies_EU2_45rm.npy')
z_raw = xr.open_dataset(PATH + 'Z500_MSLP_som_raw_All_CAT_anomalies_EU2_45rm.nc')
norm_factor = z500_mslp_factor
str_variable = "Z500_mSLP"
"""
# Z500 and TCWV
"""
data_train = np.load(PATH + 'Z500_tcwv_som_data_train_All_CAT_anomalies_EU2_45rm.npy')
data_test =  np.load(PATH + 'Z500_tcwv_som_data_test_All_CAT_anomalies_EU2_45rm.npy')
all_data = np.load(PATH + 'Z500_tcwv_som_all_data_All_CAT_anomalies_EU2_45rm.npy')
time_values = np.load(PATH +'Z500_tcwv_som_time_data_All_CAT_anomalies_EU2_45rm.npy') 
z_raw = xr.open_dataset(PATH + 'Z500_tcwv_som_raw_All_CAT_anomalies_EU2_45rm.nc')
norm_factor = z500_tcwv_factor
str_variable = "Z500_TCWV"
"""

# Z500 and MSLP ITALY
"""
dataset_str = "CERRA_LAND"
norm_factor = norm_factor_CERRA_LAND

data_train = np.load(PATH + dataset_str +  '_Italy_Z500_MSLP_som_data_train_anomalies_EU2_45rm.npy')
data_test =  np.load(PATH + dataset_str+ '_Italy_Z500_MSLP_som_data_test_anomalies_EU2_45rm.npy')
all_data = np.load(PATH + dataset_str + '_Italy_Z500_MSLP_som_all_data_anomalies_EU2_45rm.npy')
time_values = np.load(PATH +dataset_str + '_Italy_Z500_MSLP_som_time_data_anomalies_EU2_45rm.npy')
z_raw = xr.open_dataset(PATH + dataset_str + '_Italy_Z500_MSLP_som_raw_anomalies_EU2_45rm.nc')
str_variable = "Z500_mSLP"
"""
# Z500 and MSLP Sicily
dataset_str = "CERRA_LAND"
norm_factor = norm_factor_CERRA_LAND
domain_region = "Italy"
EU_domain = "EU3"

print("ATTENZIONE Stai caricando i dati di "+ domain_region + " " + EU_domain)
data_train = np.load(PATH + dataset_str +  '_' + domain_region + '_Z500_MSLP_som_data_train_anomalies_' + EU_domain + '_45rm.npy')
data_test =  np.load(PATH + dataset_str+ '_' + domain_region + '_Z500_MSLP_som_data_test_anomalies_' + EU_domain + '_45rm.npy')
all_data = np.load(PATH + dataset_str + '_' + domain_region + '_Z500_MSLP_som_all_data_anomalies_' + EU_domain + '_45rm.npy')
time_values = np.load(PATH +dataset_str + '_' + domain_region + '_Z500_MSLP_som_time_data_anomalies_' + EU_domain + '_45rm.npy')
z_raw = xr.open_dataset(PATH + dataset_str + '_' + domain_region + '_Z500_MSLP_som_raw_anomalies_' + EU_domain + '_45rm.nc')
str_variable = "Z500_mSLP"


# new extremes
# Z500 and MSLP
som_variables_str = "Z500_mSLP"
# Z500 and TCWV
#som_variables_str = "Z500_TCWV"

#dataset_str = "ARCIS"
#norm_factor = norm_factor_arcis

#dataset_str = "MSWEP"
#norm_factor = norm_factor_MSWEP

#dataset_str = "CERRA_LAND"
#norm_factor = norm_factor_CERRA_LAND

#str_variable = "Z500_mSLP"
"""
data_train = np.load(PATH + dataset_str + '_' + som_variables_str + '_som_data_train_anomalies_EU2_45rm.npy')
data_test =  np.load(PATH + dataset_str + '_' + som_variables_str + '_som_data_test_anomalies_EU2_45rm.npy')
all_data = np.load(PATH + dataset_str + '_' + som_variables_str + '_som_all_data_anomalies_EU2_45rm.npy')
time_values = np.load(PATH + dataset_str + '_' + som_variables_str + '_som_time_data_anomalies_EU2_45rm.npy')
z_raw = xr.open_dataset(PATH + dataset_str + '_' + som_variables_str + '_som_raw_anomalies_EU2_45rm.nc')

print("To check")
print(dataset_str)
print(dataset_str + '_' + som_variables_str + '_som_data_train_anomalies_EU2_45rm.npy')
print(dataset_str + '_' + som_variables_str + '_som_data_test_anomalies_EU2_45rm.npy')
print(dataset_str + '_' + som_variables_str + '_som_all_data_anomalies_EU2_45rm.npy')
print(dataset_str + '_' + som_variables_str + '_som_time_data_anomalies_EU2_45rm.npy')
print(dataset_str + '_' + som_variables_str + '_som_raw_anomalies_EU2_45rm.npy')
"""
print(data_train.shape)
len_datatrain = len(data_train[0])

time_values_test = time_values[len(data_train):]

# variable loading
#z_values = z_raw['Z'].values
#z_SOM = z_raw['Z']
lon = z_raw['lon'].values
lat = z_raw['lat'].values
nx = int((z_raw['lat'].size))
ny = int((z_raw['lon'].size))
#ndays =int((z_raw['time'].size)) #non viene usato mai

today = date.today()

print("loading " + dataset_str + '_extreme_dates_' + domain_region + '.npy')
#extreme_dates = np.load(PATH + dataset_str + '_extreme_dates.npy', allow_pickle=True) #extremes 
#extreme_dates = np.load(PATH + dataset_str + '_extreme_dates_ITALY.npy', allow_pickle=True) #extremes 
extreme_dates = np.load(PATH + dataset_str + '_extreme_dates_' + domain_region + '.npy', allow_pickle=True) #extremes 

print(len(extreme_dates))
#%% INITIALIZE THE SOM
#You want to change these to the settings that you would like. 
som_col = 5
som_row = 5
som_shape =(som_row, som_col)
min_som = min(som_col, som_row)
#x= 3 #columns
#y= 4 #row
input_length = len_datatrain #This is value is the the length of the latitude X longitude. It is the second value in the data_train.shape step. 
sigma = min_som -1        #The sigma value must be y-1. 
#sigma = 
learning_rate = 0.0005  #Learning Rate 
qerror_list = []
q_win = 100000.
number_of_soms = 10

som_names = dataset_str + "_" + domain_region + "_" + EU_domain + "_45rm_" + som_variables_str + "_" + str(today) + "_" + str(som_col) + "by" + str(som_row) + "_LR" + str(learning_rate) + "_sig" + str(sigma) + "_n_"
print(som_names)
#%% Create the SOM
for i in range(number_of_soms):   #The number of SOMs that will be generated. 
    # initialize random weights
    era5_hourly_som1 = minisom.MiniSom(som_row, som_col, input_len = input_length, sigma = sigma, learning_rate=learning_rate, neighborhood_function='gaussian', decay_function = asymptotic_decay)
    era5_hourly_som1.random_weights_init(data_train)
    # train som
    era5_hourly_som1.train(data_train, num_iteration=100000,random_order=True, verbose=True)
    q_error = era5_hourly_som1.quantization_error(data_train)
    
    #Add the details of the SOM settings into the name of the file so that you know what the SOM is showing.
    with open(folderpath + som_names +str(i+1)+'.p', 'wb') as outfile: #this is how you save the file, the str(i) is a unique name
        pickle.dump(era5_hourly_som1, outfile)
    weights = era5_hourly_som1._weights
    qerror_list += [q_error]
    i+=1
    if q_error < q_win:
        q_win = q_error
        win_weights = era5_hourly_som1
        
print('\007')

#%% reloading old soms/selecting the files 
#Z500
#som_names = "EU2_45rm_Z500_All_CAT_2024-04-23_3by2_LR0.0005_sig1_n_"
#som_names = "EU2_45rm_Z500_All_CAT_2024-04-23_3by3_LR0.0005_sig2_n"
#MSLP
#som_names= "EU2_45rm_MSLP_All_CAT_2024-04-23_3by2_LR0.0005_sig1_n"
#som_names= "EU2_45rm_MSLP_All_CAT_2024-04-23_3by3_LR0.0005_sig2_n"

#Z500 MSLP
#som_names = "EU2_45rm_Z500_mSLP_2024-05-06_3by2_LR0.0005_sig1_n_"
#som_names = "EU2_45rm_Z500_mSLP_2024-05-06_3by3_LR0.0005_sig2_n_"

#Z500 TCWV
#som_names = "EU2_45rm_Z500_TCWV_2024-05-08_3by2_LR0.0005_sig1_n_"
#som_names = "EU2_45rm_Z500_TCWV_2024-05-08_3by3_LR0.0005_sig2_n_"

# new extremes
#som_names = dataset_str + "_EU2_45rm_Z500_mSLP_2024-05-13_2by2_LR0.0005_sig1_n_"
#som_names = dataset_str + "_EU2_45rm_Z500_mSLP_2024-05-13_3by2_LR0.0005_sig1_n_"
#som_names = dataset_str + "_EU2_45rm_Z500_mSLP_2024-05-13_3by3_LR0.0005_sig2_n_"

#som_names="CERRA_LAND_Italy_EU3_45rm_Z500_mSLP_2024-06-08_3by2_LR0.0005_sig1_n"
#som_names="CERRA_LAND_EmiliaRomagna_EU2_45rm_Z500_mSLP_2024-06-07_2by2_LR0.0005_sig1_n_"
#som_names="CERRA_LAND_Italy_EU2_45rm_Z500_mSLP_2024-06-03_2by2_LR0.0005_sig1_n_"
som_names="CERRA_LAND_Italy_EU3_45rm_Z500_mSLP_2024-07-22_3by3_LR0.0005_sig2_n_"
som_names="CERRA_LAND_Italy_EU3_45rm_Z500_mSLP_2024-07-22_4by4_LR0.0005_sig3_n_"
som_names="CERRA_LAND_Italy_EU3_45rm_Z500_mSLP_2024-07-22_5by5_LR0.0005_sig4_n_"

names = ([os.path.splitext(os.path.split(x)[-1])[0] for x in glob.glob(folderpath + som_names + '*')]) #this might be different for you

#but this is just grabbing the first few characters of my names of my file (see above how I named them, for example som_8)

filepaths = glob.glob(folderpath + som_names + '*')  #this is showing the path and the given file

print(names)

#%% QE and TE
# we want to calculate QE and TE on both train and data test 
qerror_list_train = []
topoerror_list_train = []

qerror_list_test = []
topoerror_list_test = []
            
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        file = pickle.load(f) #This is loading every single som in that location
        frequencies = file.activation_response(data_test) #this is grabbing each freq
        q_error_train = round(file.quantization_error(data_train),3) #this is grabbing every q error out to 3 decimal places
        topo_error_train = round(file.topographic_error(data_train),3) #this is grabbing ever topographic error out to 3 decimal places
        qerror_list_train += [q_error_train]
        topoerror_list_train += [topo_error_train]
        
        q_error_test = round(file.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error_test = round(file.topographic_error(data_test),3) #this is grabbing ever topographic error out to 3 decimal places
        qerror_list_test += [q_error_test]
        topoerror_list_test += [topo_error_test]
        
mean_qerror_train = np.mean(qerror_list_train)
mean_qerror_test  = np.mean(qerror_list_test)

mean_terror_train = np.mean(topoerror_list_train)
mean_terror_test  = np.mean(topoerror_list_test)

index_best = np.where(qerror_list_test == np.min(qerror_list_test))[0][0]
print("qerr. best is " + names[index_best])

index_best = np.where(topoerror_list_test == np.min(topoerror_list_test))[0][0]
print("topoerr. best is " + names[index_best])
#%%
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}
                    
        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
            
        corr_list = []
        for k in range(weights.shape[0]):
            for j in range(weights.shape[1]):
                index_maps = winmap[(k,j)]
                #print(index_maps) #index of the maps of a single node
                cluster_maps = [all_data[i] for i in index_maps] # maps relative to such indices
                
                print("Node " + str(k*weights.shape[1] + j+1))
                print(" number of maps: " + str((len(cluster_maps))))
                corr_list_temp = []
                for i in range(len(cluster_maps)):
                    corr_list_temp += [np.corrcoef(weights[k,j,:], cluster_maps[i])[0,1]]
            
                print(" corr.: " + str(np.mean(corr_list_temp))) 
                corr_list += [np.mean(corr_list_temp)]
                
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        q_error = round(som.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error = round(som.topographic_error(data_test),3) 
        
        
        frame = plt.figure(figsize=(8,7))
        cs = plt.pcolormesh(freq_perc, cmap="Blues")
   
        plt.title('SOM Freq.: ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:] + ' QE =' + ' ' f"{q_error}" + ' ' + 'TE =' + ' ' f"{topo_error}", fontsize=12)

        #in the title, I am plotting every q error and topo error from each som. You need to have the f" in front and whatever variable in {}
        #And this ' ' represents a space in the title
        plt.colorbar(cs, label="freq [%]")
        plt.ylim(som_row,0) # Change the 3 to whatever size SOM you have (this is the 2nd number)
        plt.tight_layout()
        plt.xticks(ticks=[])
        plt.yticks(ticks=[])
        #plt.savefig(folderpath + dataset_str + '_all_node_' +name+'_freq.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM
        plt.show()
        
        
        
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 20),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
    
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(nx,ny,2)
                
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_mslp = SOM_array[:,:,1]  / norm_factor #Nel secondo caso è TCWV
                
                # THIS IS FOR Z500 MSLP SOMS
                levs = np.arange(-240, 241, 30)
                cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_z500,
                                  transform = ccrs.PlateCarree(),
                                  cmap="RdBu_r", levels = levs,extend='both')
                label_shaded = "anom. Z500 (m)"
                
                levels = np.arange(-20, 22, 2) # label height #mslp
                #levels = np.arange(-100, 100, 10)
                contour = axs[(k*som_col)+i].contour(lon, lat, SOM_mslp, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                """
                #THIS IS FOR Z500 TCWV SOMS
                #levs = np.arange(-150, 165, 15)
                levs = np.arange(-8, 9, 1)
                cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_mslp,
                                  transform = ccrs.PlateCarree(),
                                  cmap="RdBu_r", levels = levs,extend='both')
                label_shaded = r"anom. TCWV ($kg / m^2$)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(lon, lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                """
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)


                # Title each subplot 
                axs[(k*som_col)+i].set_title("F:" + str(int(frequencies[k,i])) + " (" + "%.2f" % freq_perc[k,i] + " %) " + r"$\rho$: %.2f" % corr_list[node] , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
        #cbar.set_label(r"anom. TCWV ($kg / m^2$)", fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        plt.savefig(folderpath + dataset_str + '_all_anomalyplot_' +name+'.png', bbox_inches='tight')
        plt.show()


#%% FIGURES ALL DATA
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        file = pickle.load(f) #This is loading every single som in that location
        frequencies = file.activation_response(all_data) #this is grabbing each freq
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        q_error = round(file.quantization_error(all_data),3) #this is grabbing every q error out to 3 decimal places
        topo_error = round(file.topographic_error(all_data),3) #this is grabbing ever topographic error out to 3 decimal places
        """
        plt.figure(figsize=(10,8))
        cs = plt.pcolormesh(freq_perc, cmap="Blues", vmin=10, vmax=20)
        plt.title(name + ' ' + 'Freq.,' + ' ' + 'QE =' + ' ' f"{q_error}" + ' ' + 'TE =' + ' ' f"{topo_error}", fontsize=12)

        #in the title, I am plotting every q error and topo error from each som. You need to have the f" in front and whatever variable in {}
        #And this ' ' represents a space in the title
        plt.colorbar(cs, label="freq [%]")
        plt.ylim(som_row,0) # Change the 3 to whatever size SOM you have (this is the 2nd number)
        plt.tight_layout()
        plt.savefig(folderpath +  dataset_str + '_frequencies_all_'+name+'.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM
        
        plt.show()
        """
        [y,E] = sammon(file.get_weights().reshape(som_col*som_row, input_length),2,display=1)

            # Plot Sammon map nodes
        plt.figure(figsize=(10,8))
        plt.scatter(y[:,0], y[:,1], s=20, c='black', marker='o', label='QE =' + ' ' f"{q_error}" + " " + 'TE =' + ' ' f"{topo_error}")
            # Add lines between nodes
        mslp = np.reshape(y,(som_row,som_col,2))
        len_x, len_y, len_z = mslp.shape

        # add vertical lines
        for i in range(len_x-1):
            for j in range(len_y):
                plt.plot(mslp[i:i+2,j,0],mslp[i:i+2,j,1],c='black')

        # add horizontal lines
        for i in range(len_x):
            for j in range(len_y-1):
                plt.plot(mslp[i,j:j+2,0],mslp[i,j:j+2,1],c='black')  

        plt.xticks([])
        plt.yticks([])
        #plt.text(1, 1, 'QE =' + ' ' f"{q_error}" )
        #plt.text(1, 10, 'TE =' + ' ' f"{topo_error}")
        plt.title("Sammonplot (all data). SOM " + som_variables_str + " " + dataset_str + " " + str(som_col) + "by" + str(som_row) + " n." + name[57:], fontsize=12)
        plt.legend()
        plt.savefig(folderpath +  dataset_str + '_all_sammonplot_'+name+'.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM

        plt.show()
        
print('\007')
print('\007')
        
#%%
"""
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        file = pickle.load(f) #This is loading every single som in that location
        frequencies = file.activation_response(data_test) #this is grabbing each freq
        print(frequencies.shape)
        print(frequencies)
        print(frequencies[0,0])
"""
        
#%%
Z500_Emilia = [[None,None],[None,None]]
Z500_Emilia = np.array(Z500_Emilia)
mslp_Emilia = [[None,None],[None,None]]
mslp_Emilia = np.array(mslp_Emilia)
#%%
Z500_Italia = [[None,None],[None,None]]
Z500_Italia = np.array(Z500_Italia)
mslp_Italia = [[None,None],[None,None]]
mslp_Italia = np.array(mslp_Italia)
#%%
Z500_diff = [[None,None],[None,None]]
Z500_diff = np.array(Z500_diff)
Z500_diff[0,0] = Z500_Emilia[0,0] - Z500_Italia[1,0]
Z500_diff[0,1] = Z500_Emilia[0,1] - Z500_Italia[1,1]
Z500_diff[1,0] = Z500_Emilia[1,0] - Z500_Italia[0,0]
Z500_diff[1,1] = Z500_Emilia[1,1] - Z500_Italia[0,1]

mslp_diff = [[None,None],[None,None]]
mslp_diff = np.array(mslp_diff)
mslp_diff[0,0] = mslp_Emilia[0,0] - mslp_Italia[1,0]
mslp_diff[0,1] = mslp_Emilia[0,1] - mslp_Italia[1,1]
mslp_diff[1,0] = mslp_Emilia[1,0] - mslp_Italia[0,0]
mslp_diff[1,1] = mslp_Emilia[1,1] - mslp_Italia[0,1]
#%%
fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                        subplot_kw={'projection': ccrs.PlateCarree()},
                        figsize=(30, 15),facecolor='white') 
fig.tight_layout()


axs=axs.flatten()
for k in range(2):
    for i in range(2):
        node=(k*som_col)+i
        
        levs = np.arange(-200, 200, 30)
        cs2=axs[(k*som_col)+i].contourf(lon, lat, Z500_diff[k,i],
                          transform = ccrs.PlateCarree(),
                          cmap="RdBu_r", levels = levs,extend='both')
        label_shaded = "anom. Z500 (m)"
        
        levels = np.arange(-20, 22, 2) # label height #mslp
        #levels = np.arange(-100, 100, 10)
        contour = axs[(k*som_col)+i].contour(lon, lat, mslp_diff[k,i], levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
        plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
        
        """
        #THIS IS FOR Z500 TCWV SOMS
        #levs = np.arange(-150, 165, 15)
        levs = np.arange(-8, 9, 1)
        cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_mslp,
                          transform = ccrs.PlateCarree(),
                          cmap="RdBu_r", levels = levs,extend='both')
        label_shaded = r"anom. TCWV ($kg / m^2$)"
        
        #levels = np.arange(-20, 22, 2) # label height #mslp
        levels = np.arange(-240, 241, 30)
        contour = axs[(k*som_col)+i].contour(lon, lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
        plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
        """
        axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

        axs[(k*som_col)+i].coastlines()
        axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
        #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)


        # Title each subplot 
        axs[(k*som_col)+i].set_title('Node:'+str(node+1) + " Freq:" + str(frequencies[k,i]) + " (" + str(freq_perc[k,i])[:5] + " %)" , fontsize=18)


        plt.tight_layout()
        fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                    wspace=0.05, hspace=0.25)

cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
#cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

cbar.set_label(label_shaded, fontsize=22)
#cbar.set_label(r"anom. TCWV ($kg / m^2$)", fontsize=22)

plt.suptitle('Difference Emilia - Italy ' + som_variables_str + ". " + dataset_str + " " , x= 0.33 ,fontsize=26)   
plt.savefig(folderpath + dataset_str + 'Difference Emilia - Italy' +name+'.png', bbox_inches='tight')
plt.show()

#%%
for path, name in zip(filepaths, names[:8]):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights
        print(weights.shape)
        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(all_data):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
        frequencies = som.activation_response(all_data)
        freq_perc = frequencies / len(all_data) * 100   # percentual freq
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 15),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
        
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(nx,ny,2)
                
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_mslp = SOM_array[:,:,1]  / norm_factor #Nel secondo caso è TCWV
                
                Z500_Emilia[k, i] = SOM_z500
                mslp_Emilia[k,i] = SOM_mslp
                # THIS IS FOR Z500 MSLP SOMS
                levs = np.arange(-240, 241, 30)
                cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_z500,
                                  transform = ccrs.PlateCarree(),
                                  cmap="RdBu_r", levels = levs,extend='both')
                label_shaded = "anom. Z500 (m)"
                
                levels = np.arange(-20, 22, 2) # label height #mslp
                #levels = np.arange(-100, 100, 10)
                contour = axs[(k*som_col)+i].contour(lon, lat, SOM_mslp, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                """
                #THIS IS FOR Z500 TCWV SOMS
                #levs = np.arange(-150, 165, 15)
                levs = np.arange(-8, 9, 1)
                cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_mslp,
                                  transform = ccrs.PlateCarree(),
                                  cmap="RdBu_r", levels = levs,extend='both')
                label_shaded = r"anom. TCWV ($kg / m^2$)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(lon, lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                """
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)


                # Title each subplot 
                axs[(k*som_col)+i].set_title('Node:'+str(node+1) + " Freq:" + str(frequencies[k,i]) + " (" + str(freq_perc[k,i])[:5] + " %)" , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
        #cbar.set_label(r"anom. TCWV ($kg / m^2$)", fontsize=22)
 
        plt.suptitle('SOM Nodes (anom. all data): ' + som_variables_str + ". " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], x= 0.33 ,fontsize=26)   
        #plt.savefig(folderpath + dataset_str + '_all_anomalyplot_' +name+'.png', bbox_inches='tight')
        plt.show()

#%%
# FIGURES DATA-TEST
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        file = pickle.load(f) #This is loading every single som in that location
        frequencies = file.activation_response(data_test) #this is grabbing each freq
        freq_perc = frequencies / len(data_test) * 100   # percentual freq
        q_error = round(file.quantization_error(data_test),3) #this is grabbing every q error out to 3 decimal places
        topo_error = round(file.topographic_error(data_test),3) #this is grabbing ever topographic error out to 3 decimal places
        """
        plt.figure(figsize=(10,8))
        cs = plt.pcolormesh(freq_perc, cmap="Blues", vmin=10, vmax=20)
        plt.title(name + ' ' + 'Freq.,' + ' ' + 'QE =' + ' ' f"{q_error}" + ' ' + 'TE =' + ' ' f"{topo_error}", fontsize=12)

        #in the title, I am plotting every q error and topo error from each som. You need to have the f" in front and whatever variable in {}
        #And this ' ' represents a space in the title
        plt.colorbar(cs, label="freq [%]")
        plt.ylim(som_row,0) # Change the 3 to whatever size SOM you have (this is the 2nd number)
        plt.tight_layout()
        plt.savefig(folderpath + '_test_frequencies_'+name+'.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM
        
        plt.show()
        """
        [y,E] = sammon(file.get_weights().reshape(som_col*som_row, input_length),2,display=1)

            # Plot Sammon map nodes
        plt.figure(figsize=(10,8))
        plt.scatter(y[:,0], y[:,1], s=20, c='black', marker='o', label='QE =' + ' ' f"{q_error}" + " " + 'TE =' + ' ' f"{topo_error}")

            # Add lines between nodes
        mslp = np.reshape(y,(som_row,som_col,2))
        len_x, len_y, len_z = mslp.shape

        # add vertical lines
        for i in range(len_x-1):
            for j in range(len_y):
                plt.plot(mslp[i:i+2,j,0],mslp[i:i+2,j,1],c='black')

        # add horizontal lines
        for i in range(len_x):
            for j in range(len_y-1):
                plt.plot(mslp[i,j:j+2,0],mslp[i,j:j+2,1],c='black')  

        plt.xticks([])
        plt.yticks([])
        plt.legend()
        plt.title("Sammonplot (test data). SOM " + som_variables_str + " " + dataset_str + " " + domain_region + " " + str(som_col) + "by" + str(som_row) + " n." + name[-2:], fontsize=12)
        plt.savefig(folderpath + dataset_str + '_test_sammonplot_'+name+'.png') #I am saving the outputs as a png file in the same file path and giving it the name of each SOM

        plt.show()
        
print('\007')
print('\007')
        

#%%
for path, name in zip(filepaths, names):
    with open (path, 'rb') as f:
        som = pickle.load(f)
        weights = som._weights

        #Need to create a new dictionary for the new data
        keys = [i for i in product(range(som_row), range(som_col))]  ## DIM OF SOMS
        winmap = {key: [] for key in keys}

        for i, x in enumerate(data_test):
            winmap[som.winner(x)].append(i)
            som_keys = getList(winmap)
        frequencies = som.activation_response(data_test)
        freq_perc = frequencies / len(data_test) * 100   # percentual freq
        datacrs = ccrs.PlateCarree()
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                figsize=(30, 15),facecolor='white') 
        fig.tight_layout()


        axs=axs.flatten()
        
#############################################################################################################################################################################################################
        
        #THIS IS VERY IMPORTANT you will multiply the "k" in the node = line by the largest value of the SOM so in the case of a 4x3 it will be multiplied by 4. For example,
        #if your SOM was an 8x5 you would multiple the k by 8.
        # not true, probably it must be multiplied by the som_col
#############################################################################################################################################################################################################

        for k in range(weights.shape[0]):
            for i in range(weights.shape[1]):
                #node = (k,i)
                node=(k*som_col)+i
                SOM_array = weights[k,i,:].reshape(nx,ny,2)
                
                SOM_z500 = SOM_array[:,:,0] / norm_factor 
                SOM_mslp = SOM_array[:,:,1]  / norm_factor 
                
    
                """
                # THIS IS FOR Z500 MSLP SOMS
                levs = np.arange(-240, 241, 30)
                cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_z500,
                                  transform = ccrs.PlateCarree(),
                                  cmap="RdBu_r", levels = levs,extend='both')
                label_shaded = "anom. Z500 (m)"
                
                levels = np.arange(-20, 22, 2) # label height #mslp
                #levels = np.arange(-100, 100, 10)
                contour = axs[(k*som_col)+i].contour(lon, lat, SOM_mslp, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
                
                """
                #THIS IS FOR Z500 TCWV SOMS
                levs = np.arange(-8, 9, 1)
                cs2=axs[(k*som_col)+i].contourf(lon, lat, SOM_mslp,
                                  transform = ccrs.PlateCarree(),
                                  cmap="RdBu_r", levels = levs,extend='both')
                label_shaded = r"anom. TCWV ($kg / m^2$)"
                
                #levels = np.arange(-20, 22, 2) # label height #mslp
                levels = np.arange(-240, 241, 30)
                contour = axs[(k*som_col)+i].contour(lon, lat, SOM_z500, levels, colors='k', transform = ccrs.PlateCarree(), linewidths=1)
                plt.clabel(contour, inline=True, fontsize=10, fmt='%1.0f') 
       
     
                axs[(k*som_col)+i].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

                axs[(k*som_col)+i].coastlines()
                axs[(k*som_col)+i].add_feature(cfeature.BORDERS) 
                #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)


                # Title each subplot 
                axs[(k*som_col)+i].set_title('Node:'+str(node+1) + " Freq:" + str(frequencies[k,i]) + " (" + str(freq_perc[k,i])[:5] + " %)" , fontsize=18)


                plt.tight_layout()
                fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                            wspace=0.05, hspace=0.25)

        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')

        cbar.set_label(label_shaded, fontsize=22)
        #cbar.set_label("Z500 (m)", fontsize=22)

        plt.suptitle('SOM Nodes (anom. test data): ' + som_variables_str + ". " + dataset_str + " " + str(som_col) + "by" + str(som_row) + " n." + name[57:], x= 0.33 ,fontsize=26)  
        plt.savefig(folderpath + dataset_str + '_test_anomalyplot_'+name+'.png', bbox_inches='tight')
        plt.show()
        
#%%



def cluster_dates_fun(data_values, time_values, som_shape, number_soms, filepath_som, som_names):
    # generate cluster index list and dates
    cluster_index_list = []
    cluster_dates_list = []
    for path, name in zip(filepath_som, som_names):
        with open (path, 'rb') as f:
            file = pickle.load(f) #This is loading every single som in that location
    # each neuron represents a cluster
            winner_coordinates = np.array([file.winner(x) for x in data_values]).T
    # with np.ravel_multi_index we convert the bidimensional
    # coordinates to a monodimensional index
            cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
            cluster_index_list += [cluster_index] 
            dim_som = som_col * som_row
            cluster_nodes_dates = divide_dates_by_index(time_values, cluster_index, dim_som)
            cluster_dates_list += [cluster_nodes_dates]

    return cluster_dates_list


def cluster_stagionality(data_values, time_values, som_shape, number_soms, filepath_som, som_names):
    # generate cluster index list and dates
    cluster_index_list = []
    cluster_dates_list = []
    month_n = np.arange(1, 13,1) # array da 1 a 12
    for path, name in zip(filepath_som, som_names):
        print(name)
        with open (path, 'rb') as f:
            file = pickle.load(f) #This is loading every single som in that location
    # each neuron represents a cluster
            winner_coordinates = np.array([file.winner(x) for x in data_values]).T
    # with np.ravel_multi_index we convert the bidimensional
    # coordinates to a monodimensional index
            cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
            cluster_index_list += [cluster_index] 
            dim_som = som_col * som_row
            cluster_nodes_dates = divide_dates_by_index(time_values, cluster_index, dim_som) 
                    #è l'array della singola som che contiene le date divise per nodi
            cluster_dates_list += [cluster_nodes_dates] 
            
            fig = plt.figure(figsize=(10,7))
            plt.title("Seasonality (all data). SOM " + som_variables_str + " " + dataset_str + " " + str(som_col) + "by" + str(som_row) + " n." + name[57:])

            for k in range(len(cluster_nodes_dates)):
                dates = np.array(cluster_nodes_dates[k])
                months = dates.astype('datetime64[M]').astype(int) % 12 + 1
                counts = np.bincount(months)[1:] # [1:] toglie il primo valore relativo al numero 0
                #print(len(counts))
                if len(counts) < 12 :
                    N = 12 - len(counts)
                    counts = np.pad(counts, (0, N), 'constant')
                    print("missing some month")
                #print(len(counts))
                print(counts)
                label_node = "#" + str(k+1)
                plt.plot(month_n,counts, "-o", label=label_node, color=colormap25_stag[k])
                
                
            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
            plt.ylabel("counts")
            plt.xlabel("month")
            plt.savefig(folderpath + dataset_str + '_all_seasonality_'+name+'.png',)
            plt.show()


def cluster_stagionality_perc(data_values, time_values, som_shape, number_soms, filepath_som, som_names, list_of_extremes):
    # generate cluster index list and dates
    cluster_index_list = []
    cluster_dates_list = []
    month_n = np.arange(1, 13,1) # array da 1 a 12
    
    # let's count how many events we have for each month
    temp_extr = list_of_extremes.astype('datetime64[M]').astype(int) % 12 + 1
    # we divide them for the month
    counts_tot_per_m = np.bincount(temp_extr)[1:] # counts of each extreme per month
    
    
    for path, name in zip(filepath_som, som_names):
        with open (path, 'rb') as f:
            file = pickle.load(f) #This is loading every single som in that location
    # each neuron represents a cluster
            winner_coordinates = np.array([file.winner(x) for x in data_values]).T
    # with np.ravel_multi_index we convert the bidimensional
    # coordinates to a monodimensional index
            cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
            cluster_index_list += [cluster_index] 
            dim_som = som_col * som_row
            cluster_nodes_dates = divide_dates_by_index(time_values, cluster_index, dim_som) 
                    #è l'array della singola som che contiene le date divise per nodi
            cluster_dates_list += [cluster_nodes_dates] 
            
            fig = plt.figure(figsize=(10,7))
            plt.title("Seasonality (all data). SOM " + som_variables_str + " " + dataset_str + " " + str(som_col) + "by" + str(som_row) + " n." + name[57:])
            iii=0
            print("SOM" + str(iii+1))
            for k in range(len(cluster_nodes_dates)):
                dates = np.array(cluster_nodes_dates[k])
                months = dates.astype('datetime64[M]').astype(int) % 12 + 1
                counts = np.bincount(months)[1:] # [1:] toglie il primo valore relativo al numero 0
                #print(counts)
                
                #print(counts_perc)
                #print(len(counts))
                if len(counts) < 12 :
                    N = 12 - len(counts)
                    print(counts)
                    counts = np.pad(counts, (0, N), 'constant')
                    print("missing some month")
                #print(len(counts))
                counts_perc = counts/counts_tot_per_m *100
                
                label_node = "#" + str(k+1)
                plt.plot(month_n,counts_perc, "-o", label=label_node, color=colormap25_stag[k])

            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
            plt.ylabel("perc [%]")
            plt.xlabel("month")
            plt.savefig(folderpath + dataset_str + '_all_seasonality_perc_'+name+'.png',)
            plt.show()
            iii = iii+1


#dates_list = cluster_dates_fun(all_data, time_values, som_shape, number_of_soms,  filepaths, names)
all_data_stagionality = cluster_stagionality(all_data, time_values, som_shape, number_of_soms,  filepaths, names)
all_data_stagionality = cluster_stagionality_perc(all_data, time_values, som_shape, number_of_soms,  filepaths, names, extreme_dates)

duration = 1000 # milliseconds
freq = 440  # Hz
winsound.Beep(freq, duration)

#%%
# stagionalità v2
# Define the colormap and number of colors
cmap_viridis = plt.get_cmap("jet")
num_colors = 25

# Extract the colors from the colormap
colors = [cmap_viridis(i / num_colors) for i in range(num_colors)]

# Example plot using the extracted colors
x = np.linspace(0, 10, 100)
fig, ax = plt.subplots()

for i, color in enumerate(colors):
    ax.plot(x, np.sin(x + i), color=color, label=f'Line {i}')

ax.legend()
plt.show()

#%%

def cluster_stagionality(data_values, time_values, som_shape, number_soms, filepath_som, som_names):
    # generate cluster index list and dates
    cluster_index_list = []
    cluster_dates_list = []
    month_n = np.arange(1, 13,1) # array da 1 a 12
    for path, name in zip(filepath_som, som_names):
        print(name)
        with open (path, 'rb') as f:
            file = pickle.load(f) #This is loading every single som in that location
    # each neuron represents a cluster
            winner_coordinates = np.array([file.winner(x) for x in data_values]).T
    # with np.ravel_multi_index we convert the bidimensional
    # coordinates to a monodimensional index
            cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
            cluster_index_list += [cluster_index] 
            dim_som = som_col * som_row
            cluster_nodes_dates = divide_dates_by_index(time_values, cluster_index, dim_som) 
                    #è l'array della singola som che contiene le date divise per nodi
            cluster_dates_list += [cluster_nodes_dates] 
            
            # colormap
            num_colors = len(cluster_nodes_dates)
            # Extract the colors from the colormap
            colors_list = [cmap_viridis(i / num_colors) for i in range(num_colors)]
            # figure
            fig = plt.figure(figsize=(10,7))
            plt.title("Seasonality (all data). SOM " + som_variables_str + " " + dataset_str + " " + str(som_col) + "by" + str(som_row) + " n." + name[57:])

            for k in range(len(cluster_nodes_dates)):
                dates = np.array(cluster_nodes_dates[k])
                months = dates.astype('datetime64[M]').astype(int) % 12 + 1
                counts = np.bincount(months)[1:] # [1:] toglie il primo valore relativo al numero 0
                #print(len(counts))
                if len(counts) < 12 :
                    N = 12 - len(counts)
                    counts = np.pad(counts, (0, N), 'constant')
                    print("missing some month")
                #print(len(counts))
                print(counts)
                label_node = "#" + str(k+1)
                plt.plot(month_n,counts, "-o", label=label_node, color=colors_list[k])
                
                
            plt.legend(loc="best")
            plt.ylabel("counts")
            plt.xlabel("month")
            plt.savefig(folderpath + dataset_str + '_all_seasonality_'+name+'.png',)
            plt.show()


def cluster_stagionality_perc(data_values, time_values, som_shape, number_soms, filepath_som, som_names, list_of_extremes):
    # generate cluster index list and dates
    cluster_index_list = []
    cluster_dates_list = []
    month_n = np.arange(1, 13,1) # array da 1 a 12
    
    # let's count how many events we have for each month
    temp_extr = list_of_extremes.astype('datetime64[M]').astype(int) % 12 + 1
    # we divide them for the month
    counts_tot_per_m = np.bincount(temp_extr)[1:] # counts of each extreme per month
    
    
    for path, name in zip(filepath_som, som_names):
        with open (path, 'rb') as f:
            file = pickle.load(f) #This is loading every single som in that location
    # each neuron represents a cluster
            winner_coordinates = np.array([file.winner(x) for x in data_values]).T
    # with np.ravel_multi_index we convert the bidimensional
    # coordinates to a monodimensional index
            cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
            cluster_index_list += [cluster_index] 
            dim_som = som_col * som_row
            cluster_nodes_dates = divide_dates_by_index(time_values, cluster_index, dim_som) 
                    #è l'array della singola som che contiene le date divise per nodi
            cluster_dates_list += [cluster_nodes_dates] 
            
            # colormap
            num_colors = len(cluster_nodes_dates)
            # Extract the colors from the colormap
            colors_list = [cmap_viridis(i / num_colors) for i in range(num_colors)]
            
            fig = plt.figure(figsize=(10,7))
            plt.title("Seasonality (all data). SOM " + som_variables_str + " " + dataset_str + " " + str(som_col) + "by" + str(som_row) + " n." + name[57:])
            iii=0
            print("SOM" + str(iii+1))
            for k in range(len(cluster_nodes_dates)):
                dates = np.array(cluster_nodes_dates[k])
                months = dates.astype('datetime64[M]').astype(int) % 12 + 1
                counts = np.bincount(months)[1:] # [1:] toglie il primo valore relativo al numero 0
                #print(counts)
                
                #print(counts_perc)
                #print(len(counts))
                if len(counts) < 12 :
                    N = 12 - len(counts)
                    print(counts)
                    counts = np.pad(counts, (0, N), 'constant')
                    print("missing some month")
                #print(len(counts))
                counts_perc = counts/counts_tot_per_m *100
                
                label_node = "#" + str(k+1)
                plt.plot(month_n,counts_perc, "-o", label=label_node, color=colors_list[k])

            plt.legend(loc="best")
            plt.ylabel("perc [%]")
            plt.xlabel("month")
            plt.savefig(folderpath + dataset_str + '_all_seasonality_perc_'+name+'.png',)
            plt.show()
            iii = iii+1

all_data_stagionality = cluster_stagionality(all_data, time_values, som_shape, number_of_soms,  filepaths, names)
all_data_stagionality = cluster_stagionality_perc(all_data, time_values, som_shape, number_of_soms,  filepaths, names, extreme_dates)

#%%
print("you need to check this")
# this are all extreme events
prova_1 = extreme_dates.astype('datetime64[M]').astype(int) % 12 + 1
# we divide them for the month
counts = np.bincount(prova_1)[1:]

# this is an example given by the function all_data_stagionality, they need to be equal
arrays = [
    [10, 12, 13,  7, 17, 18, 21, 15, 25, 32, 33, 17],
    [ 8,  7, 11, 15, 11,  8, 11, 14, 24, 44, 41, 13],
    [13, 14, 11,  8, 11, 14, 12, 18, 27, 18, 21, 13],
    [16, 16, 12,  7,  8, 17,  4,  9, 20, 22, 34, 23]
]

np_arrays = np.array(arrays)
prova_2 = np.sum(np_arrays, axis=0)
print(counts)
print(prova_2)

#%%
def SOM_maps_composites_with_ref(data_values_ref, time_values_ref, other_data_values, other_time_values, som_shape, number_soms, filepath_som, som_names):
    # generate cluster index list and dates dalla variabile di riferimento
    cluster_index_list = []
    cluster_dates_list = []
    for path, name in zip(filepath_som, som_names):
        with open (path, 'rb') as f:
            file = pickle.load(f) #This is loading every single som in that location
            print("ok1")
    # each neuron represents a cluster
            winner_coordinates = np.array([file.winner(x) for x in data_values_ref]).T
            print("ok2")
    # with np.ravel_multi_index we convert the bidimensional
    # coordinates to a monodimensional index
            cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
            print("ok3")
            cluster_index_list += [cluster_index] 
            print("ok4")
            dim_som = som_col * som_row
            print("ok5")
            cluster_nodes_dates = divide_dates_by_index(time_values_ref, cluster_index, dim_som)
            cluster_dates_list += [cluster_nodes_dates]
    
    print("ok_for")
    #genereate the maps of the second variable
    
    maps_composite = [ [None for _ in range(len(cluster_dates_list[0])) ] for i in range(number_soms)]
    for i in range(len(cluster_dates_list)):
        for j in range(len(cluster_dates_list[i])):
            list_map_temp = []
            for k in range(len(cluster_dates_list[i][j])):
                indx = np.where(other_time_values[:]==cluster_dates_list[i][j][k])[0][0]
                #print(cluster_dates_list[0][0][k])
                #print(time_values[indx])
                #print("")
                list_map_temp += [other_data_values[indx]] 
            list_map_mean = np.mean(list_map_temp, axis=0)
            maps_composite[i][j]=np.reshape(list_map_mean,(nx,ny))/norm_factor
            #print(len(list_map_temp))
    
    return maps_composite

#data_test_mslp =  np.load(PATH + 'MSLP_som_data_test_All_CAT_anomalies_EU2.npy')
all_data_mslp = np.load(PATH + 'MSLP_som_all_data_All_CAT_anomalies_EU2_45rm.npy')
time_values_mslp = np.load(PATH +'MSLP_som_time_data_All_CAT_anomalies_EU2_45rm.npy') 
#time_values_test_mslp = time_values_mslp[len(data_train):]
norm_factor = mslp_factor

all_data_composites_mslp = SOM_maps_composites_with_ref(all_data, time_values, all_data_mslp, time_values_mslp, som_shape, number_of_soms,  filepaths, names)
#data_test_composites_mslp = SOM_maps_composites_with_ref(data_test, time_values_test, data_test_mslp, time_values_test_mslp, som_shape, number_of_soms,  filepaths, names)
#%%
# CHECK BEFORE !!!!
def generate_SOM_composites_figures(data, names_som, data_type=None, dataset_name = None, test_or_all=None):
    string_variable = str(data_type)
    datacrs = ccrs.PlateCarree()
    for i in range(len(data)):    
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection':ccrs.LambertConformal(central_longitude=lon.mean(), central_latitude=lat.mean(), standard_parallels=(30, 60))},
                                figsize=(30, 15),facecolor='white') 
        """
        #You will set this to the dimensions of the SOM.  ## DIM OF SOMS
        fig, axs = plt.subplots(nrows=som_row,ncols=som_col, 
                                subplot_kw={'projection': ccrs.PlateCarree()},
                                #figsize=(30, 20),facecolor='white') #Italy NI 3X3
                                figsize=(30, 15),facecolor='white') #EU AND NI 2X3
        axs=axs.flatten()
        for k in range(len(data[i])):
            # to check
            #lev_start = SOM_mslp.min()
            #print(lev_start)             
            #lev_stop = SOM_mslp.max()
            #print(lev_stop)
            
            # mslp
            if data_type=="Z500":
                levs = np.arange(-240, 241, 30)
                cs2=axs[k].contourf(lon, lat, data[i][k],
                                  transform = ccrs.PlateCarree(),
                                  cmap="RdBu_r", levels = levs,extend='both')
            elif data_type=="mSLP":
                levs = np.arange(-20, 22, 2)
                
                n_cmap = len(levs)
                norm = mpl.colors.BoundaryNorm(levs, n_cmap)
                cmap = plt.cm.get_cmap("RdBu_r", n_cmap)
                #lev_start = -10
                #lev_step= 2
                #levs = (np.arange(lev_start-(lev_step/2), np.abs(lev_start)+(lev_step/2)+lev_step,lev_step))
                cs2=axs[k].contourf(lon, lat, data[i][k],
                                  transform = ccrs.PlateCarree(),
                                  cmap='RdBu_r', levels = levs,extend='both')
            elif data_type=="pr":
                levs = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 125, 150, 200, 250, 300, 350, 400]
                n_cmap = len(levs)
                norm = mpl.colors.BoundaryNorm(levs, n_cmap)
                cmap = plt.cm.get_cmap(precip_colormap, n_cmap)
                cs2=axs[k].contourf(lon, lat, data[i][k],
                                  n_cmap, cmap=cmap, norm=norm, transform = ccrs.PlateCarree(), extend='both')               
                
            else:
                print("CHECK your variable name")
                np_data = np.array(data)
                np_data = np_data[~np.isnan(np_data)]
                lev_start = np_data.min()
                print(lev_start)             
                lev_stop = np_data.max()
                print(lev_stop)
                step = (lev_stop-lev_start) /10
                levs = np.arange(lev_start, lev_stop, step)
                cs2=axs[k].contourf(lon, lat, data[i][k],
                                  transform = ccrs.PlateCarree(),
                                  cmap='Reds', levels = levs,extend='both')

            axs[k].set_extent([lon[0], lon[-1], lat[0], lat[-1]], ccrs.PlateCarree())

            axs[k].coastlines()
            axs[k].add_feature(cfeature.BORDERS) 
            #axs[(k*4)+i].scatter(-156.36,71.19, c='yellow',marker= 'o',s=120, linewidth=2,edgecolors= "black" ,zorder= 4,transform=datacrs)


            # Title each subplot 
            #axs[k].set_title('Node:'+str(k+1) + " Freq:" + str(frequencies[k,i]), fontsize=18)
            axs[k].set_title('Node:'+str(k+1), fontsize=18)

            plt.tight_layout()
            fig.subplots_adjust(bottom=0.25, top=0.9, left=0.05, right=0.6,
                        wspace=0.05, hspace=0.25)
        
        cbar_ax = fig.add_axes([0.08, 0.2, 0.5, 0.02])
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = np.arange(lev_start, np.abs(lev_start)+lev_step, lev_step*2),orientation='horizontal')
        #cbar=fig.colorbar(cs2,cax=cbar_ax, ticks = levs,orientation='horizontal')
        cbar = mpl.colorbar.ColorbarBase(cbar_ax, cmap=cmap, norm=norm, ticks=levs,
                                     spacing='uniform', format='%1i',  orientation='horizontal')
        
        # CHECK
        cbar.set_label(som_variables_str + " (" + unit_variable + ")", fontsize=22)
        
        plt.suptitle('Composites of ' + string_variable + " (" + dataset_name + " - " + test_or_all + " data) "+ "SOM (" + som_variables_str + ") n." + names_som[i][51:] , x= 0.33 ,fontsize=28)   
        plt.savefig(folderpath + 'Composites_'+ string_variable + "_" + dataset_name + "_" +  test_or_all + "_" +names_som[i]+ '_NI.png', bbox_inches='tight')
        plt.show()

generate_SOM_composites_figures(all_data_composites_mslp, names, "mSLP", "ERA5", test_or_all="all")


#%%
duration = 1000 # milliseconds
freq = 440  # Hz
winsound.Beep(freq, duration)


#%%


# Somma di ciascun valore all'interno di ciascun array
sums = [sum(array) for array in arrays]



#print(counts)

#%%

arrays = np.array(arrays)
sums=np.sum(arrays)
print(sums)

sums_2 = np.sum(counts)
print(sums_2)

