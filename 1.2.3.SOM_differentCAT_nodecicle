import os

os.chdir('C:\\Users\\Criss\\Documents\\Lavoro\\Assegno 2024_2025\\Codici')

#Imports 
import xarray as xr
import numpy as np
import pandas as pd
import warnings
import minisom
import pickle
from minisom import asymptotic_decay
import cartopy.mpl.ticker as cticker
from cartopy.util import add_cyclic_point
from mpl_toolkits.mplot3d import Axes3D
import matplotlib as mpl
from numpy import savetxt
from numpy import loadtxt
import cartopy
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import glob
import matplotlib.pyplot as plt
import os
from itertools import product
import winsound
from datetime import date

#Functions Used in the Code
def getList(dict):
    list = []
    for key in dict.keys():
        list.append(key)
        
    return list
def sammon(x, n, display = 2, inputdist = 'raw', maxhalves = 20, maxiter = 500, tolfun = 1e-9, init = 'default'):

    import numpy as np 
    from scipy.spatial.distance import cdist

    """Perform Sammon mapping on dataset x
    y = sammon(x) applies the Sammon nonlinear mapping procedure on
    multivariate data x, where each row represents a pattern and each column
    represents a feature.  On completion, y contains the corresponding
    co-ordinates of each point on the map.  By default, a two-dimensional
    map is created.  Note if x contains any duplicated rows, SAMMON will
    fail (ungracefully). 
    [y,E] = sammon(x) also returns the value of the cost function in E (i.e.
    the stress of the mapping).
    An N-dimensional output map is generated by y = sammon(x,n) .
    A set of optimisation options can be specified using optional
    arguments, y = sammon(x,n,[OPTS]):
       maxiter        - maximum number of iterations
       tolfun         - relative tolerance on objective function
       maxhalves      - maximum number of step halvings
       input          - {'raw','distance'} if set to 'distance', X is 
                        interpreted as a matrix of pairwise distances.
       display        - 0 to 2. 0 least verbose, 2 max verbose.
       init           - {'pca', 'cmdscale', random', 'default'}
                        default is 'pca' if input is 'raw', 
                        'msdcale' if input is 'distance'
    The default options are retrieved by calling sammon(x) with no
    parameters.
    File        : sammon.py
    Date        : 18 April 2014
    Authors     : Tom J. Pollard (tom.pollard.11@ucl.ac.uk)
                : Ported from MATLAB implementation by 
                  Gavin C. Cawley and Nicola L. C. Talbot
    Description : Simple python implementation of Sammon's non-linear
                  mapping algorithm [1].
    References  : [1] Sammon, John W. Jr., "A Nonlinear Mapping for Data
                  Structure Analysis", IEEE Transactions on Computers,
                  vol. C-18, no. 5, pp 401-409, May 1969.
    Copyright   : (c) Dr Gavin C. Cawley, November 2007.
    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program; if not, write to the Free Software
    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
    """

    # Create distance matrix unless given by parameters
    if inputdist == 'distance':
        D = x
        if init == 'default':
            init = 'cmdscale'
    else:
        D = cdist(x, x)
        if init == 'default':
            init = 'pca'

    if inputdist == 'distance' and init == 'pca':
        raise ValueError("Cannot use init == 'pca' when inputdist == 'distance'")

    if np.count_nonzero(np.diagonal(D)) > 0:
        raise ValueError("The diagonal of the dissimilarity matrix must be zero")

    # Remaining initialisation
    N = x.shape[0]
    scale = 0.5 / D.sum()
    D = D + np.eye(N)     
    if np.count_nonzero(D<=0) > 0:
        raise ValueError("Off-diagonal dissimilarities must be strictly positive")   

    Dinv = 1 / D
    if init == 'pca':
        [UU,DD,_] = np.linalg.svd(x)
        y = UU[:,:n]*DD[:n] 
    elif init == 'cmdscale':
        from cmdscale import cmdscale
        y,e = cmdscale(D)
        y = y[:,:n]
    else:
        y = np.random.normal(0.0,1.0,[N,n])
    one = np.ones([N,n])
    d = cdist(y,y) + np.eye(N)
    dinv = 1. / d
    delta = D-d 
    E = ((delta**2)*Dinv).sum() 

    # Get on with it
    for i in range(maxiter):

        # Compute gradient, Hessian and search direction (note it is actually
        # 1/4 of the gradient and Hessian, but the step size is just the ratio
        # of the gradient and the diagonal of the Hessian so it doesn't
        # matter).
        delta = dinv - Dinv
        deltaone = np.dot(delta,one)
        g = np.dot(delta,y) - (y * deltaone)
        dinv3 = dinv ** 3
        y2 = y ** 2
        H = np.dot(dinv3,y2) - deltaone - np.dot(2,y) * np.dot(dinv3,y) + y2 * np.dot(dinv3,one)
        s = -g.flatten(order='F') / np.abs(H.flatten(order='F'))
        y_old    = y

        # Use step-halving procedure to ensure progress is made
        for j in range(maxhalves):
            s_reshape = np.reshape(s, (-1,n),order='F')
            y = y_old + s_reshape
            d = cdist(y, y) + np.eye(N)
            dinv = 1 / d
            delta = D - d
            E_new = ((delta**2)*Dinv).sum()
            if E_new < E:
                break
            else:
                s = 0.5*s

        # Bomb out if too many halving steps are required
        if j == maxhalves-1:
            print('Warning: maxhalves exceeded. Sammon mapping may not converge...')

        # Evaluate termination criterion
        if abs((E - E_new) / E) < tolfun:
            if display:
                print('TolFun exceeded: Optimisation terminated')
            break

        # Report progress
        E = E_new
        if display > 1:
            print('epoch = %d : E = %12.10f'% (i+1, E * scale))

    if i == maxiter-1:
        print('Warning: maxiter exceeded. Sammon mapping may not have converged...')

    # Fiddle stress to match the original Sammon paper
    E = E * scale
    
    return [y,E]


# COLORMAP
new_colormap = [
    "#ecf5ff",     # celeste pi√π chiaro
   # "#d9ecff",    #new
    "#a1cff7",    #new
    "#5ca8e5",    #new
    "#2476b5",    #new
    "#0c4b78",    #new
    "#00334c",    #new
    "#005259",    #new
    "#008c69",    #new
    "#00cc44",
    "#95ff00",
    "#ffff00",
    "#ffd400",
    "#ffaa00",
    "#ff7f00",
    "#ff5500",
    "#ff2a00",
    "#f20c1f",
    "#cc1461",
    "#eb1cb7",    #new
    "#be21cc",
    "#8613bf",
    "#5f19a6",
    "#330067",    #new
]

precip_colormap = mpl.colors.ListedColormap(new_colormap)

precip_colormap

#%% Dataset and Export location

#State the path where the file is located. This will be the same path used in MiniSOM Tutorial Step #1
PATH ="C:/Users/Criss/Documents/Lavoro/Assegno 2024_2025/Codici/SOM/" #This is the path where the data files
folderpath = 'C:/Users/Criss/Documents/Lavoro/Assegno 2024_2025/Codici/SOM/SOMs_output/'  #The output SOMs will be stored in a seperate folder. This will need to be changed to the User's specific path and folder name

today = date.today()

#You want to change these to the settings that you would like. 
#som_col = 3
#som_row = 3

#x= 3 #columns
#y= 4 #row
#sigma = min_som -1        #The sigma value must be y-1. 
#sigma = 1
#learning_rate = 0.005  #Learning Rate 
#mean_qerror_list_cat =[]
q_win = 100000.
number_iterations = 100000
number_of_soms = 5

#best LR 0.008
#learning_rate_list = [0.00005, 0.0001, 0.0005, 0.001, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.5, 1]
#mean_qerror_list_cat = [ [None for _ in range(len(learning_rate_list)) ] for i in range(3)]
#mean_topoerror_list_cat = [ [None for _ in range(len(learning_rate_list)) ] for i in range(3)]

#sigma_list=[0.05, 0.1, 0.2, 0.5, 0.6, 0.7, 0.8, 0.9, 1]

som_col_list = [2,3,4,5]
som_row_list = [2,3,4,5]

len_list = len(som_col_list) * len(som_row_list)

mean_qerror_list_cat = [ [None for _ in range(len_list) ] for i in range(3)]
mean_topoerror_list_cat = [ [None for _ in range(len_list) ] for i in range(3)]
number_nodes = [None for _ in range(len_list) ]

learning_rate = 0.008


for i_row in range(len(som_row_list)):
    som_row = som_row_list[i_row]
    
    for i_col in range(len(som_col_list)):
        som_col = som_col_list[i_col]
        min_som = min(som_col, som_row)
        #sigma = min_som - 1
        sigma = 1
        
        number_nodes[i_col+ 4*i_row] = som_col_list[i_col] * som_row_list[i_row]
        
        qerror_list_cat = []
        topoerror_list_cat = []
        print("learning rate:" + str(learning_rate))
        
        for n in range(3):
            qerror_list = []
            topoerror_list = []
            
            catn = n + 1
            print("CAT" + str(catn) + ": ")
            print(" charact. of the SOM:")
            print("  " + str(som_col) + "by" + str(som_row) + "       n_iter.:" + str(number_iterations))
            print("  LR:" + str(learning_rate) + "  sig:" + str(sigma))
            
            data_train = np.load(PATH + 'TEST2_som_data_train_CAT'+ str(catn) + 'norm.npy')
            time_values = np.load(PATH +'TEST2_som_time_data_CAT'+ str(catn) + 'norm.npy')
            z_raw = xr.open_dataset(PATH + 'VER2_SOM_Z_raw_CAT'+ str(catn) + 'norm.nc')
            print(data_train.shape)
            len_datatrain = len(data_train[0])
            input_length = len_datatrain #This is value is the the length of the latitude X longitude. It is the second value in the data_train.shape step. 
            
            # variable loading
            #z_values = z_raw['Z'].values
            #z_SOM = z_raw['Z']
            lon = z_raw['lon'].values
            lat = z_raw['lat'].values
            nx = int((z_raw['lat'].size))
            ny = int((z_raw['lon'].size))
            ndays =int((z_raw['time'].size))
            
            norm_factor = [0.10014513220331027, 0.10251989701956438, 0.1285688367470829]
            norm_factor = norm_factor[n]
            
            som_names = "VARIABLE_SOM_CAT"+ str(catn) + "_" + str(today) + "_" + str(som_col) + "by" + str(som_row) + "_LR" + str(learning_rate) + "_sig" + str(sigma) + "_iter" + str(number_iterations) +  "_n_"
            print(som_names)
            
            # generate the soms
            for i in range(number_of_soms):   #The number of SOMs that will be generated. 
                # initialize random weights
                era5_hourly_som1 = minisom.MiniSom(som_row, som_col, input_len = input_length, sigma = sigma, learning_rate=learning_rate, neighborhood_function='gaussian', decay_function = asymptotic_decay)
                era5_hourly_som1.random_weights_init(data_train)
                # train som
                era5_hourly_som1.train(data_train, num_iteration= number_iterations,random_order=True, verbose=True)
                q_error = era5_hourly_som1.quantization_error(data_train)
                
                #Add the details of the SOM settings into the name of the file so that you know what the SOM is showing.
                with open(folderpath + som_names +str(i+1)+'.p', 'wb') as outfile: #this is how you save the file, the str(i) is a unique name
                    pickle.dump(era5_hourly_som1, outfile)
                weights = era5_hourly_som1._weights
                qerror_list += [q_error]
                i+=1
                if q_error < q_win:
                    q_win = q_error
                    win_weights = era5_hourly_som1
            
            qerror_list_cat += [qerror_list]
    
            print('\007')
            
            names = ([os.path.splitext(os.path.split(x)[-1])[0] for x in glob.glob(folderpath + som_names + '*')]) #this might be different for you
    
            #but this is just grabbing the first few characters of my names of my file (see above how I named them, for example som_8)
    
            filepaths = glob.glob(folderpath + som_names + '*')  #this is showing the path and the given file
    
            print(names)
            
            for path, name in zip(filepaths, names):
                with open (path, 'rb') as f:
                    file = pickle.load(f) #This is loading every single som in that location
                    frequencies = file.activation_response(data_train) #this is grabbing each freq
                    q_error = round(file.quantization_error(data_train),3) #this is grabbing every q error out to 3 decimal places
                    topo_error = round(file.topographic_error(data_train),6) #this is grabbing ever topographic error out to 3 decimal places
                    print(topo_error)
                    topoerror_list += [topo_error]
                    
            topoerror_list_cat += [topoerror_list]
            
            print('\007')
    
        
                    
            print ("End CAT" + str(catn))
            
        
        mean_qerror_list_cat[0][i_col+ 4*i_row] = np.mean(qerror_list_cat[0])
        mean_qerror_list_cat[1][i_col+ 4*i_row] = np.mean(qerror_list_cat[1])
        mean_qerror_list_cat[2][i_col+ 4*i_row] = np.mean(qerror_list_cat[2])
        
    
        mean_topoerror_list_cat[0][i_col+ 4*i_row] = np.mean(topoerror_list_cat[0])
        mean_topoerror_list_cat[1][i_col+ 4*i_row] = np.mean(topoerror_list_cat[1])
        mean_topoerror_list_cat[2][i_col+ 4*i_row] = np.mean(topoerror_list_cat[2])
        


#%% Saving data and figures 

#list_values having strings  
#learning_rate_list
QE_CAT1 = mean_qerror_list_cat[0]
QE_CAT2 = mean_qerror_list_cat[1]
QE_CAT3 = mean_qerror_list_cat[2]

TE_CAT1 = mean_topoerror_list_cat[0]
TE_CAT2 = mean_topoerror_list_cat[1]
TE_CAT3 = mean_topoerror_list_cat[2]

# LEARNING RATE
#dict = {'Learning Rate':learning_rate_list, 'QE CAT1':QE_CAT1, 'QE CAT2':QE_CAT2, 'QE CAT3':QE_CAT3, 'TE CAT1':TE_CAT1, 'TE CAT2':TE_CAT2, 'TE CAT3':TE_CAT3}  
# SIGMA
dict = {'NumberNodes':number_nodes, 'QE CAT1':QE_CAT1, 'QE CAT2':QE_CAT2, 'QE CAT3':QE_CAT3, 'TE CAT1':TE_CAT1, 'TE CAT2':TE_CAT2, 'TE CAT3':TE_CAT3}  
df = pd.DataFrame(dict)  
print(df)  

#df.to_csv(folderpath + "QE_TE_variable_LR_" + str(som_col) + "by" + str(som_row) + ".csv", index=False)
df.to_csv(folderpath + "QE_TE_variable_nodes_" + str(som_col) + "by" + str(som_row) + "_sig1.csv", index=False)


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))
#fig.suptitle("Variable LR:" + str(som_col) + "by" + str(som_row) + "  sig." + str(sigma) + "  n iter." + str(number_iterations), size=15)
fig.suptitle("Variable nodes:  LR" + str(learning_rate) + "n iter." + str(number_iterations), size=15)
ax1.plot(number_nodes, QE_CAT1, label="CAT1", linestyle = "--", marker = "o", markersize = 5)
ax1.plot(number_nodes, QE_CAT2, label="CAT2", linestyle = "--", marker = "o", markersize = 5)
ax1.plot(number_nodes, QE_CAT3, label="CAT3", linestyle = "--", marker = "o", markersize = 5)
#ax1.set_xscale("log")
ax1.set_title("QE error")
ax1.set_xlabel("nodes")
ax1.set_ylabel("error")
ax1.legend()

ax2.plot(number_nodes, TE_CAT1, label="CAT1", linestyle = "--", marker = "o", markersize = 5)
ax2.plot(number_nodes, TE_CAT2, label="CAT2", linestyle = "--", marker = "o", markersize = 5)
ax2.plot(number_nodes, TE_CAT3, label="CAT3", linestyle = "--", marker = "o", markersize = 5)
#ax2.set_xscale("log")
ax2.set_title("TE error")
ax2.set_xlabel("nodes")
#ax2.set_ylabel("TE")
ax2.legend()
plt.savefig(folderpath + "QE_TE_variable_nodes_sig1_1")


fig, ((ax1, ax2), (ax3, ax)) = plt.subplots(2, 2, figsize=(10,10))
#fig.suptitle("Variable LR:" + str(som_col) + "by" + str(som_row) + "  sig." + str(sigma) + "  n iter." + str(number_iterations), size=15)
fig.suptitle("Variable nodes:  LR" + str(learning_rate) + "n iter." + str(number_iterations), size=15)
ax1.plot(number_nodes, QE_CAT1, label="CAT1", linestyle = "--", marker = "o", markersize = 5)
#ax1.plot(sigma_list, QE_CAT2, label="CAT2", linestyle = "--", marker = "o", markersize = 5)
#ax1.plot(sigma_list, QE_CAT3, label="CAT3", linestyle = "--", marker = "o", markersize = 5)
ax1.set_title("QE error - CAT1")
ax1.set_xlabel("nodes")
ax1.set_ylabel("error")
ax1.legend()

#ax2.plot(sigma_list, QE_CAT1, label="CAT1", linestyle = "--", marker = "o", markersize = 5)
ax2.plot(number_nodes, QE_CAT2, label="CAT2", linestyle = "--", marker = "o", markersize = 5, color="orange")
#ax1.plot(sigma_list, QE_CAT3, label="CAT3", linestyle = "--", marker = "o", markersize = 5)
ax2.set_title("QE error - CAT2")
ax2.set_xlabel("nodes")
#ax2.set_ylabel("error")
ax2.legend()

#ax1.plot(sigma_list, QE_CAT1, label="CAT1", linestyle = "--", marker = "o", markersize = 5)
#ax1.plot(sigma_list, QE_CAT2, label="CAT2", linestyle = "--", marker = "o", markersize = 5)
ax3.plot(number_nodes, QE_CAT3, label="CAT3", linestyle = "--", marker = "o", markersize = 5, color="green")
ax3.set_title("QE error - CAT3")
ax3.set_xlabel("nodes")
ax3.set_ylabel("error")
ax3.legend()

ax.plot(number_nodes, TE_CAT1, label="CAT1", linestyle = "--", marker = "o", markersize = 5)
ax.plot(number_nodes, TE_CAT2, label="CAT2", linestyle = "--", marker = "o", markersize = 5)
ax.plot(number_nodes, TE_CAT3, label="CAT3", linestyle = "--", marker = "o", markersize = 5)
#ax2.set_xscale("log")
ax.set_title("TE error")
ax.set_xlabel("nodes")
#ax2.set_ylabel("TE")
ax.legend()
   
plt.tight_layout() 
    
plt.savefig(folderpath + "QE_TE_variable_nodes_sig1_2")
